{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE with disentanglement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "auto encodeur variationnel avec demelage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==========================================================¶\n",
    "Date : 06 dec. 2018\n",
    "MS Valdom > apprenants > omar attaf, laurent lapasset, didier le picaut\n",
    "Version = 2.0\n",
    "=========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllabus - auto encoder (AE)\n",
    "# http://www.xavierdupre.fr/app/ensae_teaching_dl/helpsphinx//chapters/deep_apprentissage_sans_labels.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que sont les auto-encodeurs\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Awesome-Pytorch-list\n",
    "#https://github.com/bharathgs/Awesome-pytorch-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ml-cheatsheet.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# == EXECUTIVE SUMMARY:\n",
    ". [Epoch = 200, average loss= 0.001] vs [Epoch = 1, average loss = 0.0016]\n",
    ". perte d'information ~ 0.1%\n",
    ". temps instance  ~ 6 mn\n",
    ". pas necessaire de faire 200 Epochs (?)\n",
    ". average loss = 0.001 pour Epoch = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# autocompletion\n",
    "# =================\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debut du decompte du temps\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : library cuda est-elle presente ?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : presence du framework cudnn ?\n",
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9232e87ad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproductibilite\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrer a la main (ne pas passer par argparse)\n",
    "batchsize=BATCH_SIZE= 128\n",
    "EPOCHS = 15\n",
    "loginterval =LOG_INTERVAL = 10\n",
    "CUDA = True\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaration de la var d'espace latent\n",
    "ZDIMS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances de DataLoader chargent les tenseurs in cuda-memory \n",
    "kwargs = {'num_workers': 11, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation des dataset train et test a partir de Mnist\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaration du modele VAE\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "# encodeur\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc21 = nn.Linear(400, ZDIMS)\n",
    "        \n",
    "        self.fc22 = nn.Linear(400, ZDIMS)\n",
    "\n",
    "# decodeur        \n",
    "        self.fc3 = nn.Linear(ZDIMS, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "# sorties(mu, logvar): \n",
    "# moy et var, une pour chaque dimension latente, \n",
    "\n",
    "    def encode(self, x: Variable) -> (Variable, Variable):\n",
    "        h1 = self.relu(self.fc1(x))  \n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "# sur train\n",
    "# - prenez mu, var appris pour chacune des dimensions de ZDIMS et tirez échantillon\n",
    "# aléatoire de cette distribution    \n",
    "# - l'ensemble du réseau est formé de sorte que ces échantillons dessinés au \n",
    "# hasard se décodent en sortie qui ressemble à l'entrée    \n",
    "# - ce qui signifie que mu, var sera appris des distributions qui codent \n",
    "# les entrées    \n",
    "# - en raison du terme KLD supplémentaire (voir loss_function () \n",
    "# ci-dessous), la distribution aura tendance à unifier les Gaussiens     \n",
    "\n",
    "    \n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "        if self.training: \n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: Variable) -> Variable:\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        return self.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "model = VAE()\n",
    "if cuda:\n",
    "    model.cuda()   \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD est la deviation Kullback-Leibler\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar) -> Variable:\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "# Normaliser par le même nombre d'éléments que dans la reconstruction \n",
    "    KLD /= BATCH_SIZE * 784\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(epoch):\n",
    "   \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "       \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        losses.append(loss.cpu().item())\n",
    "        \n",
    "        loss.backward()\n",
    "       \n",
    "        \n",
    "        #train_loss += loss.data[0]\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                #loss.data[0] \n",
    "                loss.item()/ len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "  \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if CUDA:\n",
    "        \n",
    "        # make sure this lives on the GPU\n",
    "                data = data.cuda()\n",
    "\n",
    "            data = Variable(data, volatile=False)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "        #test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "        \n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "          \n",
    "                save_image(comparison.data.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.005489\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.003201\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.002365\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.002178\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.002024\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.002085\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.002037\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.001958\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.001932\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.001850\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.001833\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.001804\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.001734\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.001621\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.001672\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.001603\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.001636\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.001566\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.001584\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.001525\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.001468\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.001503\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.001548\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.001445\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.001467\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.001464\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.001417\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.001376\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.001475\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.001374\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.001422\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.001409\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.001358\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.001347\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.001343\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.001336\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.001357\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.001290\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.001357\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.001306\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.001306\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.001361\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.001364\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.001301\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.001250\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001321\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.001305\n",
      "====> Epoch: 1 Average loss: 0.0016\n",
      "====> Test set loss: 0.0012\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001237\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.001284\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.001216\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.001280\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.001231\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001307\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.001269\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.001235\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.001231\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.001232\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.001175\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.001234\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.001296\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.001249\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.001225\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001232\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.001236\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.001201\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.001241\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.001205\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.001183\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.001195\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.001263\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.001191\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.001243\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.001234\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.001196\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.001201\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.001207\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001196\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.001214\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.001237\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.001160\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.001191\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.001208\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.001188\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.001189\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.001159\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.001173\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.001174\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.001216\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.001171\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.001170\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.001162\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.001148\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.001212\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001213\n",
      "====> Epoch: 2 Average loss: 0.0012\n",
      "====> Test set loss: 0.0011\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001138\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.001200\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.001169\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.001229\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.001164\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001154\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.001231\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.001138\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.001168\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.001185\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001118\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.001168\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001129\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.001115\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001122\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.001127\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.001155\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.001129\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.001153\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.001088\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001114\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.001112\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.001166\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.001145\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001171\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.001172\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.001130\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.001181\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.001152\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.001115\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.001083\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.001171\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001097\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.001134\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.001070\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.001159\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.001147\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.001147\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.001127\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.001113\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.001164\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.001133\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.001106\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.001131\n",
      "====> Epoch: 3 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001132\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.001098\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.001118\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.001096\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.001110\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.001091\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.001136\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.001139\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.001123\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.001174\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.001106\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.001129\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.001102\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.001120\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.001146\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001099\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.001161\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.001100\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.001070\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.001124\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001136\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.001114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.001117\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.001100\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.001126\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.001108\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.001112\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.001087\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.001112\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.001121\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001085\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.001115\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001161\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.001101\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.001090\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.001113\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.001102\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.001105\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.001131\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.001098\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001143\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.001120\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.001105\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.001098\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.001102\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001089\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.001091\n",
      "====> Epoch: 4 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001078\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.001060\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.001085\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.001098\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.001119\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001110\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.001083\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.001101\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001085\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.001094\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001153\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.001105\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.001084\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.001126\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.001144\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.001155\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.001049\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.001096\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.001087\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.001085\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.001096\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.001092\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.001043\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.001063\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.001122\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.001120\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.001117\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.001121\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.001108\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.001068\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.001099\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.001042\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.001044\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.001098\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.001134\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.001050\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.001083\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.001150\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.001105\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.001117\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001088\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.001074\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.001085\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.001094\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.001086\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.001125\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.001093\n",
      "====> Epoch: 5 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.001074\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.001074\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.001116\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.001063\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.001083\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001102\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.001116\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.001133\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.001080\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.001082\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.001057\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.001066\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.001041\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.001058\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.001083\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.001084\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.001098\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.001076\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.001090\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.001048\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001085\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.001122\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.001093\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.001102\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.001027\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.001115\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.001091\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.001074\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.001046\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.001052\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001103\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.001096\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.001066\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.001100\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.001045\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.001111\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.001098\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.001079\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.001104\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.001099\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001086\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.001067\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.001088\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.001043\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.001105\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.001088\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.001096\n",
      "====> Epoch: 6 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001070\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.001096\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.001103\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.001065\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.001046\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001049\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.001114\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.001069\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.001062\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.001091\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.001088\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.001086\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.001047\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.001111\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.001071\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001083\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.001089\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.001072\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.001095\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.001063\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.001098\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.001063\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.001079\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.001098\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.001080\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001093\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.001036\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.001062\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.001032\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.001073\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.001042\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.001051\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.001106\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.001089\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.001061\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001055\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.001085\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.001094\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.001071\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.001113\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.001103\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001070\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.001092\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.001036\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.001046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001082\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.001098\n",
      "====> Epoch: 7 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001092\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.001138\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.001075\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.001089\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.001046\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001053\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.001053\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.001077\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.001094\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.001070\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001096\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.001105\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.001064\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.001055\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.001078\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001056\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.001067\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.001070\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.001068\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.001033\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001012\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.001077\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.001035\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.001048\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.001090\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001098\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.001106\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.001061\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.001090\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.001060\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.001062\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.001038\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.001083\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.001098\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.001054\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001096\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.001074\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.001078\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.001076\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.001095\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.001005\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.001081\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.001021\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.001067\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.001087\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001106\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.001031\n",
      "====> Epoch: 8 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001030\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.001049\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.001103\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.001100\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.001090\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001074\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.001061\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.001037\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.001035\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.001083\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001103\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.001058\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.001083\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.001047\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.001017\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.001071\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.001080\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.001031\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.001054\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.001062\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001055\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.001074\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.001074\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.001035\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.001020\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001096\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.001038\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.001038\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.001109\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.001042\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001047\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.001075\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.001085\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.001092\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.001050\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001076\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.001022\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.001070\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.001050\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.001062\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.001070\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.001062\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.001029\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.001045\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.001087\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001084\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.001095\n",
      "====> Epoch: 9 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001072\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.001086\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.001062\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.001088\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.001047\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001016\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.001019\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.001064\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.001090\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.001056\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001090\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.001070\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.001034\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.001066\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.001068\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001035\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.001079\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.001061\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.001059\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.001069\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001068\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.001090\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.001092\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.001085\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.001073\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001078\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.001038\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.001049\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.001046\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.001067\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.001038\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.001043\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.001059\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.001043\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.001069\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.001064\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.001080\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.001058\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.001036\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.001063\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001077\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.001021\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.001040\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.001075\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.001069\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001073\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.001038\n",
      "====> Epoch: 10 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.001053\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.001024\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.001065\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.001073\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.001058\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.001020\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.001073\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.001079\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.001029\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.001058\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.001040\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.001067\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.001074\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.001040\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.001122\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.001066\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.001098\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.001019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.001095\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.001023\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.001042\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.001061\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.001044\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.001062\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.001050\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.001046\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.001052\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.001052\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.001076\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.001075\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.001078\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.001072\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.001090\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.001065\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.001049\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.001058\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.001093\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.001054\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.000971\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.001056\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.001044\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.001040\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.001069\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.001021\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.001059\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.001045\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.001053\n",
      "====> Epoch: 11 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.001055\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.001020\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.001009\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.001062\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.001072\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.001066\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.001071\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.001058\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.001072\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.001066\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.001021\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.001022\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.001037\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.001081\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.001076\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.001031\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.001041\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.001048\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.001029\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.001074\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001083\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.001056\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.001045\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.001046\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.001056\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.001028\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.001021\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.001035\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.001039\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.001079\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.001075\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.001036\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.001033\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.001043\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.001046\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.001047\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.001052\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.001040\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.001074\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.001071\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.001084\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.001113\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.001068\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.001049\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.001042\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.001020\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.001031\n",
      "====> Epoch: 12 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.001099\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.001079\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.001069\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.001039\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.001041\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.001065\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.001028\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.001059\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.001053\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.001037\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.001086\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.001064\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.001037\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.001053\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.001010\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.001033\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.001043\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.001049\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.001032\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.001060\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.001033\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.001053\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.001064\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.001042\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.001045\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001044\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.001025\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.001033\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.001024\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.001071\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.000989\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.001060\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.001064\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.001041\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.001030\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.001052\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.001076\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.001022\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.001066\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.001005\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.001037\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.001056\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.001020\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.001089\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.001062\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.001042\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.001061\n",
      "====> Epoch: 13 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001042\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.001028\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.001065\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.001069\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.001041\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.001025\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.001028\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.001038\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.001052\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.001054\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.001037\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.001035\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.001076\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.001075\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.001034\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.001045\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.001038\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.001085\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.001061\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.001064\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.001025\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.001036\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.001028\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.001022\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.001046\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.001042\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.001025\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.001027\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.001031\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.001091\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.001034\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.001057\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.001068\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.001026\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.001048\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.001037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.001072\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.001065\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.001060\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.001025\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.001058\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.001044\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.001066\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.001053\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.001056\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.001043\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.001014\n",
      "====> Epoch: 14 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.001043\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 0.001023\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.001072\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 0.001060\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.001014\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.001078\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.001036\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 0.001042\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.001040\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 0.001044\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.001070\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 0.001037\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.001034\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 0.001047\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.001005\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.001043\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.001052\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 0.001050\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.001033\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 0.001057\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.001042\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 0.001043\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.001025\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 0.001069\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.001035\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.001063\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.001042\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 0.001058\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.001102\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 0.001053\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.001067\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 0.001057\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.001050\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 0.001025\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.001045\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.001025\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.001020\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 0.001054\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.001060\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 0.001033\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.001037\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 0.001077\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 0.001072\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 0.001047\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.001002\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.001009\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.001021\n",
      "====> Epoch: 15 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "# 64 ensembles de vecteurs ZDIMS-float aléatoires, soit 64 emplacements / MNIST   \n",
    "    sample = Variable(torch.randn(64, ZDIMS))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if CUDA:\n",
    "            sample = sample.cuda()\n",
    "            sample = model.decode(sample).cpu()\n",
    "            \n",
    "            save_image(sample.data.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(sample.view(64, 1, 28, 28), 'results/sample_continuous.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f91d8ec5d30>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the gradient mean value for each FC layer across training\n",
    "#from torch.autograd import Variable\n",
    "#for batch_idx, (data, _) in enumerate(train_loader):\n",
    "#   gradient, *_ = train_loader.dataset\n",
    "#    #print(f\"Gradient of w{batch_idx} w.r.t to L: {gradient}\")\n",
    "#plt.plot(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.autograd import Variable\n",
    "#import torch\n",
    "#x = Variable(torch.FloatTensor(train_loader), requires_grad=True)\n",
    "## or we can directly backprop using loss\n",
    "#loss.backward() # equivalent to loss.backward(torch.FloatTensor([1.0]))\n",
    "#print(x.grad.data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d execution : 33.0 secondes ---\n"
     ]
    }
   ],
   "source": [
    "# Affichage du temps d execution\n",
    "print(\"Temps d execution : %s secondes ---\" % round((time.time() - start_time),1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
