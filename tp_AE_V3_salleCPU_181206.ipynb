{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE with disentanglement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "auto encodeur variationnel avec demelage"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "==========================================================¶\n",
    "Date : 06 dec. 2018\n",
    "MS Valdom : équipe Omar Attaf, Laurent Lapasset, Didier Le Picaut\n",
    "Version = 3.0\n",
    "=========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllabus - auto encoder (AE)\n",
    "# http://www.xavierdupre.fr/app/ensae_teaching_dl/helpsphinx//chapters/deep_apprentissage_sans_labels.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que sont les auto-encodeurs\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Awesome-Pytorch-list\n",
    "#https://github.com/bharathgs/Awesome-pytorch-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ml-cheatsheet.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# == EXECUTIVE SUMMARY:\n",
    ". [Epoch = 200, average loss= 0.001] vs [Epoch = 1, average loss = 0.0016]\n",
    ". perte d'information ~ 0.1%\n",
    ". temps instance  ~ 6 mn\n",
    ". pas necessaire de faire 200 Epochs (?)\n",
    ". average loss = 0.001 pour Epoch = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# autocompletion\n",
    "# =================\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debut du decompte du temps\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : library cuda est-elle presente ?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : presence du framework cudnn ?\n",
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe2e0034730>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproductibilite\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrer a la main (ne pas passer par argparse)\n",
    "batchsize=BATCH_SIZE= 128\n",
    "EPOCHS = 15\n",
    "loginterval =LOG_INTERVAL = 10\n",
    "CUDA = True\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaration de la var d'espace latent\n",
    "ZDIMS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances de DataLoader chargent les tenseurs in cuda-memory \n",
    "kwargs = {'num_workers': 11, 'pin_memory': True} if cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation des dataset train et test a partir de Mnist\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaration du modele VAE\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "# encodeur\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc21 = nn.Linear(400, ZDIMS)\n",
    "        \n",
    "        self.fc22 = nn.Linear(400, ZDIMS)\n",
    "\n",
    "# decodeur        \n",
    "        self.fc3 = nn.Linear(ZDIMS, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "# sorties(mu, logvar): \n",
    "# moy et var, une pour chaque dimension latente, \n",
    "\n",
    "    def encode(self, x: Variable) -> (Variable, Variable):\n",
    "        h1 = self.relu(self.fc1(x))  \n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "# sur train\n",
    "# - prenez mu, var appris pour chacune des dimensions de ZDIMS et tirez échantillon\n",
    "# aléatoire de cette distribution    \n",
    "# - l'ensemble du réseau est formé de sorte que ces échantillons dessinés au \n",
    "# hasard se décodent en sortie qui ressemble à l'entrée    \n",
    "# - ce qui signifie que mu, var sera appris des distributions qui codent \n",
    "# les entrées    \n",
    "# - en raison du terme KLD supplémentaire (voir loss_function () \n",
    "# ci-dessous), la distribution aura tendance à unifier les Gaussiens     \n",
    "\n",
    "    \n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "        if self.training: \n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: Variable) -> Variable:\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "# l'utilisation de la sigmoid est adaptée à nos données car nous manipulons en entrée des images N&B donc on \n",
    "# a des valeurs (après normalisation entre 0 et 1) , et la forme de la sigmoide convient bien ici\n",
    "        return self.sigmoid(self.fc4(h3)) \n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "model = VAE()\n",
    "model.to(device)   \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLD est la deviation Kullback-Leibler\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar) -> Variable:\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "# Normaliser par le même nombre d'éléments que dans la reconstruction \n",
    "    KLD /= BATCH_SIZE * 784\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(epoch):\n",
    "   \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "\n",
    "        ########################################\n",
    "        # Pour conserver les valeurs du gradient\n",
    "        # Ne marche pas à l'ENSEEITH : \n",
    "        # RuntimeError: the derivative for 'target' is not implemented with binary_cross_entropy\n",
    "        # data.requires_grad = True\n",
    "        ########################################\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        ########################################\n",
    "        # Pour conserver les valeurs du gradient\n",
    "        # Ne marche pas à l'ENSEEITH\n",
    "        # Récupérer la valeur de data.grad et la conserver dans un tableau Tab_Gradient\n",
    "        ########################################        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "       \n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        losses.append(loss.cpu().item())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #train_loss += loss.data[0]\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                #loss.data[0] \n",
    "                loss.item()/ len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "  \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            # on force le device par détecté\n",
    "            data = data.to(device)\n",
    "\n",
    "            data = Variable(data, volatile=False)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "        \n",
    "            #test_loss += loss_function(recon_batch, data, mu, logvar).data[0]\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "        \n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "          \n",
    "                save_image(comparison.data.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.001203\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.001195\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.001154\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.001138\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.001182\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.001142\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.001195\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.001167\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.001118\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.001157\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.001154\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.001141\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.001154\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.001162\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.001114\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.001141\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.001185\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.001139\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.001143\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.001154\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.001138\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.001118\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.001146\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.001140\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.001149\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.001167\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.001169\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.001107\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.001124\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.001109\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.001140\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.001143\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.001128\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.001119\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.001145\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.001119\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.001097\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.001128\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.001115\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.001121\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.001165\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.001110\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.001095\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.001114\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.001105\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.001141\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.001171\n",
      "====> Epoch: 1 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.001074\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.001120\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.001116\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.001144\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.001112\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.001101\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.001121\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.001115\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.001139\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.001113\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.001106\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.001118\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.001121\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.001092\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.001085\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.001139\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.001136\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.001070\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.001069\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.001102\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.001113\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.001102\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.001150\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.001135\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.001077\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.001093\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.001143\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.001079\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.001141\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001152\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.001103\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.001128\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.001142\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.001104\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.001093\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.001087\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.001106\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.001118\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.001107\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.001119\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.001118\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.001106\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.001075\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.001086\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.001122\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.001133\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.001121\n",
      "====> Epoch: 2 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001139\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.001084\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.001085\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.001098\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.001122\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001102\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.001101\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.001120\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.001105\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.001084\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001088\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.001123\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.001091\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.001104\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.001064\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.001071\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.001090\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.001089\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.001127\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.001087\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.001102\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.001116\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.001074\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.001090\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001128\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.001098\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.001072\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.001108\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.001058\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.001080\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.001060\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.001055\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.001085\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.001068\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001110\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.001066\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.001086\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.001082\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.001060\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.001105\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.001072\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.001078\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.001097\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.001109\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.001097\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.001102\n",
      "====> Epoch: 3 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.001076\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.001097\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.001083\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.001099\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.001042\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.001054\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.001082\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.001114\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.001074\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.001060\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.001063\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.001091\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.001071\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.001069\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.001093\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.001092\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.001101\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.001093\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.001069\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.001040\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.001096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.001054\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.001118\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.001072\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.001081\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.001066\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.001047\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.001046\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.001056\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.001090\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.001116\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.001154\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.001093\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.001108\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.001093\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.001096\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.001091\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.001081\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.001051\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.001052\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.001061\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.001070\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.001094\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.001096\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.001074\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001090\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.001089\n",
      "====> Epoch: 4 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001040\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.001066\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.001063\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.001064\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.001090\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.001065\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.001077\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.001067\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.001056\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.001120\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.001052\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.001055\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.001127\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.001060\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.001082\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.001055\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.001084\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.001093\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.001086\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.001132\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.001086\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.001066\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.001088\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.001096\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.001035\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.001116\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.001078\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.001093\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.001097\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.001051\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.001114\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.001066\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.001076\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.001070\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.001093\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.001078\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.001078\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.001089\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.001079\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.001079\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001083\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.001037\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.001040\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.001074\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.001110\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.001068\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.001042\n",
      "====> Epoch: 5 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.001063\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.001093\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.001061\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.001099\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.001070\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.001085\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.001088\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.001113\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.001080\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.001076\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.001065\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.001045\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.001047\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.001081\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.001089\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.001066\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.001056\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.001094\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.001083\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.001045\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001069\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.001054\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.001117\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.001047\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.001061\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.001034\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.001019\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.001026\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.001074\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.001084\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.001022\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.001066\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.001041\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.001111\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.001064\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.001103\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.001046\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.001056\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.001105\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.001059\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.001057\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.001078\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.001089\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.001070\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.001037\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.001071\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.001088\n",
      "====> Epoch: 6 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.001103\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.001074\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.001046\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.001050\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.001074\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.001026\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.001015\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.001083\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.001032\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.001094\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.001102\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.001062\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.001062\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.001046\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.001079\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.001062\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.001077\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.001044\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.001104\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.001108\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.001046\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.001091\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.001095\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.001065\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.001063\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.001095\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.001092\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.001054\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.001044\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.001011\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.001056\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.001050\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.001070\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.001029\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.001048\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.001036\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.001063\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.001053\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.001081\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.001033\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.001048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001063\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.001069\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.001053\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.001041\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.001041\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.001077\n",
      "====> Epoch: 7 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001033\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.001067\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.001058\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.001063\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.001053\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001067\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.001057\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.001070\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.001041\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.001084\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001073\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.001018\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.001028\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.001066\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.001069\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001078\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.001066\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.001051\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.001052\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.001049\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001050\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.001060\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.001077\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.001053\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.001028\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001025\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.001050\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.001037\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.001037\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.001080\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.001069\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.001091\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.001040\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.001043\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.001057\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001072\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.001081\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.001018\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.001048\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.001067\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.001093\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.001067\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.001073\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.001086\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.001076\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.001053\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.001032\n",
      "====> Epoch: 8 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.001058\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.001056\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.001107\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.001076\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.001084\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.001072\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.001045\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.001050\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.001048\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.001088\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.001081\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.001061\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.001035\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.001071\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.001079\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.001066\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.001049\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.001065\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.001022\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.001074\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001065\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.001091\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.001049\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.001022\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.001071\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.001070\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.001062\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.001069\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.001082\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.001085\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001032\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.001060\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.001015\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.001018\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.001050\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001036\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.001024\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.001053\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.001055\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.001043\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.001060\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.001058\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.001038\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.001027\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.001076\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.001086\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.001065\n",
      "====> Epoch: 9 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001057\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.001005\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.001028\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.001053\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.001054\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001065\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.001067\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.001057\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.001068\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.001068\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.001061\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.001011\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.001051\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.001056\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.001063\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001048\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.001087\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.001055\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.001060\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.001081\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001080\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.001057\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.001037\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.001032\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.001048\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001045\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.001057\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.001053\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.001096\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.001071\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.001054\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.001055\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.001056\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.001025\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.001062\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.001061\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.001056\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.001078\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.001044\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.001074\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.001018\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.001067\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.001038\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.001074\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.001052\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001049\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.001035\n",
      "====> Epoch: 10 Average loss: 0.0011\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.001043\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.001048\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.001051\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.001038\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.001038\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.001042\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.001078\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.001055\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.001035\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.000999\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.001080\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.001023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.001049\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.001027\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.001036\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.001054\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.001048\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.001058\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.001046\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.001014\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.001056\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.001022\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.001060\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.001064\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.001037\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.001060\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.001067\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.001109\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.001050\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.001007\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.001062\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.001050\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.001019\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.001063\n",
      "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.001049\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.001041\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.001045\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.001035\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.001067\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.001072\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.001049\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.001048\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.001044\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.001042\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.001044\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.001039\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.001059\n",
      "====> Epoch: 11 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.001056\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.001053\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.001049\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.001035\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.000998\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.001038\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.001036\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.001014\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.001072\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.001063\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.001024\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.001058\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.001016\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.001025\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.001034\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.001051\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.001002\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.001084\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.001066\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.001056\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.001032\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.001051\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.001089\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.001042\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.001028\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.001064\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.001046\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.001032\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.001070\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.001058\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.001032\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.001036\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.001075\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.001063\n",
      "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.001039\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.000999\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.001028\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.001001\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.001034\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.001043\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.001035\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.001047\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.001059\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.001056\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.001045\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.001044\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.001031\n",
      "====> Epoch: 12 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.001044\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.001021\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.001039\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.001057\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.001027\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.001038\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.001020\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.001062\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.001076\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.001038\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.001063\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.001072\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.001047\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.001011\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.001038\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.001057\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.001055\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.001029\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.001035\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.001051\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.001057\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.001082\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.001080\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.001073\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.001031\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001013\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.001032\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.001019\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.001032\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.001050\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.001073\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.001060\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.001056\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.001039\n",
      "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.001070\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.001053\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.001034\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.001071\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.001057\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.001065\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.001065\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.001029\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.001046\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.001060\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.001021\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.001027\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.000987\n",
      "====> Epoch: 13 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.001058\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.000996\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.001043\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.001025\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.001036\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.001073\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.001065\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.001040\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.001038\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.001041\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.001040\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.001046\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.001083\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.001036\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.001012\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.001029\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.001046\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.001045\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.001062\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.001040\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.000999\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.001031\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.001050\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.001054\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.001047\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000984\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.001036\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.001042\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.001051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.001029\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.001061\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.001031\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.001067\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.001016\n",
      "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.001091\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.001081\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.001053\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.001002\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.001057\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.001030\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.001038\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.001037\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.001036\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.001061\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.001049\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.001075\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.001041\n",
      "====> Epoch: 14 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.001068\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 0.001057\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.001045\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 0.001028\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.001095\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.001044\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.001043\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 0.001036\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.001036\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 0.001035\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.001025\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 0.001021\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.001065\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 0.001029\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.001058\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.001062\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.001039\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 0.001041\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.001093\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 0.001062\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.001013\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 0.001013\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.000997\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 0.001054\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.001037\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.001050\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.001067\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 0.001061\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.001047\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 0.001066\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.001064\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 0.001047\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.001076\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 0.001075\n",
      "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.001023\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.001075\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.001032\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 0.001027\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.001074\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 0.001039\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.001003\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 0.000985\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 0.001024\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 0.001060\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.001035\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.001041\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.001038\n",
      "====> Epoch: 15 Average loss: 0.0010\n",
      "====> Test set loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "# 64 ensembles de vecteurs ZDIMS-float aléatoires, soit 64 emplacements / MNIST   \n",
    "    sample = Variable(torch.randn(64, ZDIMS))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            sample = sample.to(device)\n",
    "            sample = model.decode(sample).cpu()\n",
    "            \n",
    "            save_image(sample.data.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_image(sample.view(64, 1, 28, 28), 'results/sample_continuous.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe2947b3be0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Affichage du gradient\n",
    "# Ne marche pas à l'ENSEEITH\n",
    "# RuntimeError: the derivative for 'target' is not implemented with binary_cross_entropy\n",
    "# \n",
    "# Méthode : \n",
    "# - normaliser les valeurs du tableau Tab_Gradient construit pendant l'opération de train\n",
    "# - ploter le tableau       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://www.kdnuggets.com/2018/04/getting-started-pytorch-understanding-automatic-differentiation.html/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot the gradient mean value for each FC layer across training\n",
    "#from torch.autograd import Variable\n",
    "#for batch_idx, (data, _) in enumerate(train_loader):\n",
    "#   gradient, *_ = train_loader.dataset\n",
    "#    #print(f\"Gradient of w{batch_idx} w.r.t to L: {gradient}\")\n",
    "#plt.plot(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/43451125/pytorch-what-are-the-gradient-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.autograd import Variable\n",
    "#import torch\n",
    "#x = Variable(torch.FloatTensor(train_loader), requires_grad=True)\n",
    "## or we can directly backprop using loss\n",
    "#loss.backward() # equivalent to loss.backward(torch.FloatTensor([1.0]))\n",
    "#print(x.grad.data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d execution : 33.0 secondes ---\n"
     ]
    }
   ],
   "source": [
    "# Affichage du temps d execution\n",
    "print(\"Temps d execution : %s secondes ---\" % round((time.time() - start_time),1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
