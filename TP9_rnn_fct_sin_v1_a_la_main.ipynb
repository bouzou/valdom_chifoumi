{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================¶\n",
    "# Date : 14 dec. 2018\n",
    "# MS Valdom > apprenants > omar attaf, laurent lapasset, didier le picaut\n",
    "# version = 1.0\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================\n",
    "# objectif : Réseaux de neurones récurrents\n",
    "#================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations cachées :\n",
    "# ----------------------\n",
    "#. En réseau traditionnel, toutes les entrées / sorties sont indépendantes les unes des autres. \n",
    "# Dans certaines modélisations de données, vous voulez vous rappeler quelles données sont\n",
    "# arrivées juste avant (comme dans le traitement du langage naturel)\n",
    "# Les RNN ont donc une \"mémoire\" interne qui est mise à jour chaque fois que le réseau est appelé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment formons-nous un RNN?\n",
    "# ----------------------\n",
    "# la retropropagation à travers le temps > nous considérons une version non déroulée du NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment comprendre le LSTM\n",
    "# Versions améliorées\n",
    "# ----------------------\n",
    "# Le RNN de base présente un certain nombre de pièges: Puisque nous propagons les gradients \n",
    "# sur plusieurs itérations, le gradient aura tendance à disparaître / exploser car le gradient\n",
    "# dans la premiere couche est une multiplication du gradient sur l'autre couche. \n",
    "# Les mémoires LSTM (mémoire à court et long terme) ont été conçues pour permettre l’entraînement\n",
    "# d’un réseau profond et récurrent en contrôlant ce qui est conservé / oublié \n",
    "# à chaque itération GRU (unité récurrente gated) est une simplification récente du \n",
    "# LSTM qui semble aussi facile à former, mais plus efficace en calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment comprendre le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application \n",
    "# ----------------------\n",
    "# Nous allons utiliser un simple RNN pour predire la fonction \"sin ()\"\n",
    "\n",
    "# Utilisez :\n",
    "# ----------------------  \n",
    "\n",
    "# nn.RNNCell (1, 10), \n",
    "# nn.Linear (10, 1)\n",
    "# nn.MSELoss ()\n",
    " \n",
    "# Créez un DataSet personnalisé, qui retourne:\n",
    "# ----------------------    \n",
    "# x = np.array (range (1000)) + np.random.randint (-100, 100, 1)\n",
    "# np.sin (x / 50.). astype ('float32')\n",
    "\n",
    "# N'oubliez pas:\n",
    "# ----------------------\n",
    "# Initialise l'état caché à zéro\n",
    "# Tracer les pertes et la fonction sin () réelle générée par le rnn\n",
    "# Regardez torch.stack () pour concaténer chaque sortie de l'itération RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf pckg autograd\n",
    "# https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debut du decompte du temps\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créez un DataSet personnalisé, qui retourne:\n",
    "# ----------------------    \n",
    "x = np.array (range (1000)) + np.random.randint (-100, 100, 1)\n",
    "y = np.sin (x / 50.). astype ('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# courbe y=sin(x):\n",
    "# ---------------------- \n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y=sin(x)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamètres du modèle\n",
    "# ----------------------  \n",
    "dtype = torch.FloatTensor\n",
    "input_size = 7      # taille de la couche d’entrée\n",
    "hidden_size = 6     # 6+1=7\n",
    "output_size = 1     # sortie ychap\n",
    "epochs = 200        # nn Epochs\n",
    "seq_length = 200    # longueur séquence d’entrée et cible.\n",
    "lr = 0.05           # learming rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data train, où x = séquence d'entrée et y = séquence cible.\n",
    "# ---------------------- \n",
    "data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "data = np.sin(data_time_steps)\n",
    "data.resize((seq_length + 1, 1))\n",
    "x = Variable(torch.Tensor(data[:-1]).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.Tensor(data[1:]).type(dtype), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrice des poids des neurones w1, w2\n",
    "# ---------------------- \n",
    "\n",
    "w1 = torch.FloatTensor(input_size, hidden_size).type(dtype)\n",
    "init.normal(w1, 0.0, 0.4)\n",
    "w1 =  Variable(w1, requires_grad=True)\n",
    "\n",
    "w2 = torch.FloatTensor(hidden_size, output_size).type(dtype)\n",
    "init.normal(w2, 0.0, 0.3)\n",
    "w2 = Variable(w2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# méthode forward (passe avant)\n",
    "# ---------------------- \n",
    "def forward(input, context_state, w1, w2):\n",
    "  xh = torch.cat((input, context_state), 1)\n",
    "  context_state = torch.tanh(xh.mm(w1))\n",
    "  out = context_state.mm(w2)\n",
    "  return  (out, context_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# méthode forward\n",
    "# ---------------------- \n",
    "# arguments : vecteur entree context_state 2 mat poids\n",
    "# vecteur xh en concaténant vecteur d’entrée avec vecteur context_state. \n",
    "# produit scalaire entre vecteur xh et w1, \n",
    "# tanh comme non-linéarité (mieux avec les RNN)\n",
    "# produit scalaire contexte_state et w2.\n",
    "# sortie context_state comme entree au prochain pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele RNN\n",
    "# ---------------------- \n",
    "for i in range(epochs):\n",
    "  total_loss = 0\n",
    "  context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=True)\n",
    "    \n",
    "  for j in range(x.size(0)):\n",
    "    input = x[j:(j+1)]\n",
    "    target = y[j:(j+1)]\n",
    "    (pred, context_state) = forward(input, context_state, w1, w2)\n",
    "    loss = (pred - target).pow(2).sum()/2\n",
    "    total_loss += loss\n",
    "    loss.backward()\n",
    "    w1.data -= lr * w1.grad.data\n",
    "    w2.data -= lr * w2.grad.data\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "    context_state = Variable(context_state.data)\n",
    "    \n",
    "  if i % 10 == 0:\n",
    "     print(\"Epoch: {} loss {}\".format(i, total_loss.data[0]))\n",
    "\n",
    "\n",
    "context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=False)\n",
    "predictions = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modele RNN = le train\n",
    "# ---------------------- \n",
    "# La boucle externe itère sur Epoch \n",
    "# Au début de chaque Epoch, initialiser context_state avec des zéros.\n",
    "# La boucle interne parcourt chaque élément de la séquence.\n",
    "# Nous exécutons la méthode forward qui renvoie la prédiction\n",
    "# et le contexte_state qui seront utilisés pour la prochaine étape temporelle. \n",
    "# Ensuite, nous calculons l’erreur quadratique moyenne (MSE), \n",
    "# En backward sur la perte, on calcule les gradients, puis maj poids. \n",
    "# effacer les gradients à chaque itération en appelant la méthode zero_ (), sinon les gradients accumulés. \n",
    "# envelopper le vecteur context_state dans la nouvelle variable, pour le détacher de son historique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire la prediction\n",
    "# ---------------------- \n",
    "for i in range(x.size(0)):\n",
    "  input = x[i:i+1]\n",
    "  (pred, context_state) = forward(input, context_state, w1, w2)\n",
    "  context_state = context_state\n",
    "  predictions.append(pred.data.numpy().ravel()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire la prediction\n",
    "# ---------------------- \n",
    "# À chaque étape de la séquence, alimenter le modèle avec un seul point de données (cf courbe)\n",
    "# et prédire une valeur au prochain pas de temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualiser le Y et Ychapeau\n",
    "# ----------------------  \n",
    "pl.scatter(data_time_steps[:-1], x.data.numpy(), s=100, label=\"Y\")\n",
    "pl.scatter(data_time_steps[1:], predictions, label=\"Ychapeau\")\n",
    "pl.legend()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse : \n",
    "# ---------------------- \n",
    "# avec epochs = 200 et seq_length = 200, Ychapeau predit bien la vrai valeur Y=sin(x)\n",
    "# plus on augmente seq_length et + on nourrit en nb d Epochs meilleure sera la prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du temps d execution\n",
    "print(\"Temps d execution : %s minutes --- secondes\" % round((time.time() - start_time)/60),2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
