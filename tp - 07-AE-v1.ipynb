{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='ae.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Date : 29 nov. 2018\n",
    "# MS Valdom > apprenants > omar attaf, laurent lapasset, didier le picaut\n",
    "# Version = 1.0\n",
    "# ========================================================================="
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# == EXECUTIVE SUMMARY: \n",
    ".dataset mnist = train = 60 000 images 28x28 + test = 10 000 images 28 x 28\n",
    ".bien régler la taille du batch selon puissance GPU\n",
    ".avec 200 Epochs, en test, loss criteria = 63.6% - 5mn de temps traitment\n",
    ".modele rx neurones : 2 couches fully connected + 1 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllabus - auto encoder (AE)\n",
    "# http://www.xavierdupre.fr/app/ensae_teaching_dl/helpsphinx//chapters/deep_apprentissage_sans_labels.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://reyhaneaskari.github.io/AE.html\n",
    "# -------------------------------------------\n",
    "- Les encodeurs automatiques sont l’un des modèles d’apprentissage en profondeur \n",
    "non supervisé.\n",
    "- L'objectif d'un encodeur automatique est la réduction de la dimensionnalité \n",
    "et la découverte des fonctionnalités.\n",
    "- Un encodeur automatique est formé pour prévoir sa propre entrée, \n",
    "mais pour empêcher le modèle d’apprendre le mappage d’identité, \n",
    "certaines contraintes sont appliquées aux unités cachées."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/# \n",
    "# ----------------------------------\n",
    "Qu'est-ce qu'un autoencoder?\n",
    "L’idée générale de l’auto-encodeur (AE) est de faire passer les informations par un étroit goulet d’étranglement entre les parties encodeur (entrée) et décodeur (sortie) en miroir d’un réseau de neurones. (voir le schéma ci-dessous)\n",
    "\n",
    "Etant donné que l'architecture et la fonction de perte du réseau sont configurées de manière à ce que la sortie tente d'émuler l'entrée, le réseau doit apprendre à coder les données d'entrée sur l'espace très limité représenté par le goulot d'étranglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# autocompletion\n",
    "# =================\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les libraries\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== mise en place d un compteur de temps pour voir le temps de convergence du Rx neurones [trop  ou pas ?]\n",
    "# Debut du decompte du temps\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : library cuda est-elle presente ?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : presence du framework cudnn ?\n",
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ffb74749f90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproductible\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "\n",
    "#epochs=10\n",
    "epochs=200\n",
    "\n",
    "loginterval=10\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== data chargement > dataset train + dataset test\n",
    "# Téléchargez ou chargez le jeu de données MNIST téléchargé \n",
    "# mélangez les données à chaque époque \n",
    "\n",
    "\n",
    "# Les instances de DataLoader chargeront les tenseurs directement dans la mémoire du GPU \n",
    "\n",
    "kwargs = {'num_workers': 11, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# dataset train\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)\n",
    "\n",
    "# dataset test\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== transform=transforms.ToTensor()) ?\n",
    "\n",
    "# Convertit une image PIL ou numpy.ndarray (H x L x C) \n",
    "# comprise dans l'intervalle [0, 255] en une torche.\n",
    "# Tensoriseur de forme (C x H x W) compris dans l'intervalle [0.0, 1.0].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== creation du modele\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "# ENCODEUR \n",
    "# 28 x 28 pixels = 784 pixels en entrée, 400 sorties\n",
    "# couche unitaire linéaire rectifiée de 400 à 400 \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400,20) # espace latent = couche cachee\n",
    "            )\n",
    "\n",
    "# DECODEUR \n",
    "# de goulot d' étranglement à 400 caché\n",
    "# on fait le travail autre-sens\n",
    "    \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(20,400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400,784),\n",
    "            )\n",
    "\n",
    "        # create using nn.Sequential()\n",
    "        # encoder :FullyConnected (784 -> 400), RELU activation; FC (400, 20)\n",
    "        # decoder :FullyConnected (20 -> 400), RELU activation; FC (400, 784)\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return torch.sigmoid(self.decoder(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xview = x.view((-1, 784))\n",
    "        z = self.encode(xview)\n",
    "        return self.decode(z)\n",
    "\n",
    "model = AE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.view.html\n",
    "# otimizer utilise = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_batch, x):\n",
    "    \n",
    "    # compute bce as the binary cross entropy across the batch as a sum\n",
    "    # L'entrée est binarisée et Binary Cross Entropy a été utilisé comme fonction de perte.\n",
    "    # dans quelle mesure l'entrée x et la sortie recon_x sont-elles d'accord?\n",
    "    \n",
    "    loss = torch.nn.BCELoss(reduction='sum')\n",
    "    return loss(recon_batch, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1,784))\n",
    "        losses.append(loss.cpu().item())\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % loginterval == 0:\n",
    "            print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.1f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)), end='')\n",
    "\n",
    "    print(' Average loss: {:.1f}'.format(\n",
    "          train_loss / len(train_loader.dataset)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch = model(data)\n",
    "            test_loss += loss_function(recon_batch, data.view(-1, 784))\n",
    "\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batchsize, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(' Test loss: {:.1f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results/reconstruction = les images sont reconstruites et images stockees ds le fichier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1 [58880/60000 (98%)]\tLoss: 90.7 Average loss: 132.0 Test loss: 89.0\n",
      " Train Epoch: 2 [58880/60000 (98%)]\tLoss: 84.4 Average loss: 83.4 Test loss: 78.1\n",
      " Train Epoch: 3 [58880/60000 (98%)]\tLoss: 76.9 Average loss: 77.0 Test loss: 74.5\n",
      " Train Epoch: 4 [58880/60000 (98%)]\tLoss: 76.2 Average loss: 74.2 Test loss: 72.6\n",
      " Train Epoch: 5 [58880/60000 (98%)]\tLoss: 73.3 Average loss: 72.4 Test loss: 71.4\n",
      " Train Epoch: 6 [58880/60000 (98%)]\tLoss: 69.6 Average loss: 71.1 Test loss: 70.2\n",
      " Train Epoch: 7 [58880/60000 (98%)]\tLoss: 69.8 Average loss: 70.2 Test loss: 69.4\n",
      " Train Epoch: 8 [58880/60000 (98%)]\tLoss: 71.2 Average loss: 69.4 Test loss: 68.8\n",
      " Train Epoch: 9 [58880/60000 (98%)]\tLoss: 68.4 Average loss: 68.8 Test loss: 68.5\n",
      " Train Epoch: 10 [58880/60000 (98%)]\tLoss: 69.2 Average loss: 68.3 Test loss: 68.1\n",
      " Train Epoch: 11 [58880/60000 (98%)]\tLoss: 67.3 Average loss: 67.8 Test loss: 67.7\n",
      " Train Epoch: 12 [58880/60000 (98%)]\tLoss: 69.2 Average loss: 67.4 Test loss: 67.3\n",
      " Train Epoch: 13 [58880/60000 (98%)]\tLoss: 66.8 Average loss: 67.1 Test loss: 67.1\n",
      " Train Epoch: 14 [58880/60000 (98%)]\tLoss: 64.8 Average loss: 66.8 Test loss: 66.9\n",
      " Train Epoch: 15 [58880/60000 (98%)]\tLoss: 64.3 Average loss: 66.5 Test loss: 66.6\n",
      " Train Epoch: 16 [58880/60000 (98%)]\tLoss: 66.8 Average loss: 66.3 Test loss: 66.3\n",
      " Train Epoch: 17 [58880/60000 (98%)]\tLoss: 66.6 Average loss: 66.1 Test loss: 66.3\n",
      " Train Epoch: 18 [58880/60000 (98%)]\tLoss: 67.8 Average loss: 65.9 Test loss: 66.1\n",
      " Train Epoch: 19 [58880/60000 (98%)]\tLoss: 64.0 Average loss: 65.7 Test loss: 66.0\n",
      " Train Epoch: 20 [58880/60000 (98%)]\tLoss: 64.6 Average loss: 65.5 Test loss: 65.8\n",
      " Train Epoch: 21 [58880/60000 (98%)]\tLoss: 64.3 Average loss: 65.4 Test loss: 65.7\n",
      " Train Epoch: 22 [58880/60000 (98%)]\tLoss: 65.9 Average loss: 65.2 Test loss: 65.6\n",
      " Train Epoch: 23 [58880/60000 (98%)]\tLoss: 64.0 Average loss: 65.1 Test loss: 65.5\n",
      " Train Epoch: 24 [58880/60000 (98%)]\tLoss: 64.9 Average loss: 65.0 Test loss: 65.5\n",
      " Train Epoch: 25 [58880/60000 (98%)]\tLoss: 65.8 Average loss: 64.8 Test loss: 65.4\n",
      " Train Epoch: 26 [58880/60000 (98%)]\tLoss: 64.7 Average loss: 64.8 Test loss: 65.3\n",
      " Train Epoch: 27 [58880/60000 (98%)]\tLoss: 65.1 Average loss: 64.6 Test loss: 65.4\n",
      " Train Epoch: 28 [58880/60000 (98%)]\tLoss: 64.2 Average loss: 64.6 Test loss: 65.1\n",
      " Train Epoch: 29 [58880/60000 (98%)]\tLoss: 61.5 Average loss: 64.5 Test loss: 65.0\n",
      " Train Epoch: 30 [58880/60000 (98%)]\tLoss: 65.2 Average loss: 64.4 Test loss: 65.1\n",
      " Train Epoch: 31 [58880/60000 (98%)]\tLoss: 63.5 Average loss: 64.3 Test loss: 65.0\n",
      " Train Epoch: 32 [58880/60000 (98%)]\tLoss: 63.7 Average loss: 64.2 Test loss: 65.0\n",
      " Train Epoch: 33 [58880/60000 (98%)]\tLoss: 64.8 Average loss: 64.2 Test loss: 64.9\n",
      " Train Epoch: 34 [58880/60000 (98%)]\tLoss: 65.7 Average loss: 64.1 Test loss: 64.8\n",
      " Train Epoch: 35 [58880/60000 (98%)]\tLoss: 60.8 Average loss: 64.0 Test loss: 64.9\n",
      " Train Epoch: 36 [58880/60000 (98%)]\tLoss: 66.6 Average loss: 64.0 Test loss: 64.9\n",
      " Train Epoch: 37 [58880/60000 (98%)]\tLoss: 59.2 Average loss: 63.9 Test loss: 64.7\n",
      " Train Epoch: 38 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 63.8 Test loss: 64.7\n",
      " Train Epoch: 39 [58880/60000 (98%)]\tLoss: 63.7 Average loss: 63.8 Test loss: 64.6\n",
      " Train Epoch: 40 [58880/60000 (98%)]\tLoss: 65.4 Average loss: 63.7 Test loss: 64.5\n",
      " Train Epoch: 41 [58880/60000 (98%)]\tLoss: 62.6 Average loss: 63.7 Test loss: 64.6\n",
      " Train Epoch: 42 [58880/60000 (98%)]\tLoss: 64.5 Average loss: 63.6 Test loss: 64.5\n",
      " Train Epoch: 43 [58880/60000 (98%)]\tLoss: 64.2 Average loss: 63.6 Test loss: 64.5\n",
      " Train Epoch: 44 [58880/60000 (98%)]\tLoss: 60.8 Average loss: 63.6 Test loss: 64.4\n",
      " Train Epoch: 45 [58880/60000 (98%)]\tLoss: 64.7 Average loss: 63.5 Test loss: 64.5\n",
      " Train Epoch: 46 [58880/60000 (98%)]\tLoss: 65.5 Average loss: 63.5 Test loss: 64.4\n",
      " Train Epoch: 47 [58880/60000 (98%)]\tLoss: 64.3 Average loss: 63.4 Test loss: 64.5\n",
      " Train Epoch: 48 [58880/60000 (98%)]\tLoss: 63.3 Average loss: 63.4 Test loss: 64.5\n",
      " Train Epoch: 49 [58880/60000 (98%)]\tLoss: 64.3 Average loss: 63.4 Test loss: 64.3\n",
      " Train Epoch: 50 [58880/60000 (98%)]\tLoss: 61.6 Average loss: 63.3 Test loss: 64.3\n",
      " Train Epoch: 51 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 63.3 Test loss: 64.3\n",
      " Train Epoch: 52 [58880/60000 (98%)]\tLoss: 61.2 Average loss: 63.2 Test loss: 64.3\n",
      " Train Epoch: 53 [58880/60000 (98%)]\tLoss: 61.4 Average loss: 63.2 Test loss: 64.2\n",
      " Train Epoch: 54 [58880/60000 (98%)]\tLoss: 63.1 Average loss: 63.2 Test loss: 64.2\n",
      " Train Epoch: 55 [58880/60000 (98%)]\tLoss: 64.1 Average loss: 63.2 Test loss: 64.2\n",
      " Train Epoch: 56 [58880/60000 (98%)]\tLoss: 63.1 Average loss: 63.1 Test loss: 64.3\n",
      " Train Epoch: 57 [58880/60000 (98%)]\tLoss: 62.7 Average loss: 63.1 Test loss: 64.2\n",
      " Train Epoch: 58 [58880/60000 (98%)]\tLoss: 65.8 Average loss: 63.0 Test loss: 64.2\n",
      " Train Epoch: 59 [58880/60000 (98%)]\tLoss: 63.2 Average loss: 63.0 Test loss: 64.3\n",
      " Train Epoch: 60 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 63.0 Test loss: 64.2\n",
      " Train Epoch: 61 [58880/60000 (98%)]\tLoss: 62.6 Average loss: 63.0 Test loss: 64.1\n",
      " Train Epoch: 62 [58880/60000 (98%)]\tLoss: 64.5 Average loss: 62.9 Test loss: 64.2\n",
      " Train Epoch: 63 [58880/60000 (98%)]\tLoss: 62.8 Average loss: 62.9 Test loss: 64.2\n",
      " Train Epoch: 64 [58880/60000 (98%)]\tLoss: 64.1 Average loss: 62.9 Test loss: 64.1\n",
      " Train Epoch: 65 [58880/60000 (98%)]\tLoss: 61.7 Average loss: 62.9 Test loss: 64.1\n",
      " Train Epoch: 66 [58880/60000 (98%)]\tLoss: 63.9 Average loss: 62.8 Test loss: 64.1\n",
      " Train Epoch: 67 [58880/60000 (98%)]\tLoss: 61.0 Average loss: 62.8 Test loss: 64.1\n",
      " Train Epoch: 68 [58880/60000 (98%)]\tLoss: 65.8 Average loss: 62.8 Test loss: 64.1\n",
      " Train Epoch: 69 [58880/60000 (98%)]\tLoss: 63.2 Average loss: 62.8 Test loss: 64.0\n",
      " Train Epoch: 70 [58880/60000 (98%)]\tLoss: 62.4 Average loss: 62.8 Test loss: 64.1\n",
      " Train Epoch: 71 [58880/60000 (98%)]\tLoss: 60.7 Average loss: 62.7 Test loss: 64.0\n",
      " Train Epoch: 72 [58880/60000 (98%)]\tLoss: 62.9 Average loss: 62.7 Test loss: 64.0\n",
      " Train Epoch: 73 [58880/60000 (98%)]\tLoss: 61.5 Average loss: 62.7 Test loss: 64.0\n",
      " Train Epoch: 74 [58880/60000 (98%)]\tLoss: 62.7 Average loss: 62.7 Test loss: 64.0\n",
      " Train Epoch: 75 [58880/60000 (98%)]\tLoss: 62.5 Average loss: 62.7 Test loss: 64.0\n",
      " Train Epoch: 76 [58880/60000 (98%)]\tLoss: 61.6 Average loss: 62.6 Test loss: 64.0\n",
      " Train Epoch: 77 [58880/60000 (98%)]\tLoss: 62.3 Average loss: 62.6 Test loss: 64.0\n",
      " Train Epoch: 78 [58880/60000 (98%)]\tLoss: 63.1 Average loss: 62.6 Test loss: 63.9\n",
      " Train Epoch: 79 [58880/60000 (98%)]\tLoss: 64.5 Average loss: 62.6 Test loss: 64.0\n",
      " Train Epoch: 80 [58880/60000 (98%)]\tLoss: 60.7 Average loss: 62.6 Test loss: 63.9\n",
      " Train Epoch: 81 [58880/60000 (98%)]\tLoss: 60.3 Average loss: 62.5 Test loss: 63.9\n",
      " Train Epoch: 82 [58880/60000 (98%)]\tLoss: 64.2 Average loss: 62.5 Test loss: 63.9\n",
      " Train Epoch: 83 [58880/60000 (98%)]\tLoss: 63.4 Average loss: 62.5 Test loss: 63.9\n",
      " Train Epoch: 84 [58880/60000 (98%)]\tLoss: 64.4 Average loss: 62.5 Test loss: 63.9\n",
      " Train Epoch: 85 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 62.5 Test loss: 64.1\n",
      " Train Epoch: 86 [58880/60000 (98%)]\tLoss: 61.0 Average loss: 62.5 Test loss: 64.0\n",
      " Train Epoch: 87 [58880/60000 (98%)]\tLoss: 65.8 Average loss: 62.4 Test loss: 63.9\n",
      " Train Epoch: 88 [58880/60000 (98%)]\tLoss: 62.9 Average loss: 62.4 Test loss: 63.9\n",
      " Train Epoch: 89 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 62.4 Test loss: 63.8\n",
      " Train Epoch: 90 [58880/60000 (98%)]\tLoss: 62.6 Average loss: 62.4 Test loss: 63.9\n",
      " Train Epoch: 91 [58880/60000 (98%)]\tLoss: 64.4 Average loss: 62.4 Test loss: 63.8\n",
      " Train Epoch: 92 [58880/60000 (98%)]\tLoss: 65.6 Average loss: 62.4 Test loss: 63.9\n",
      " Train Epoch: 93 [58880/60000 (98%)]\tLoss: 58.4 Average loss: 62.4 Test loss: 63.9\n",
      " Train Epoch: 94 [58880/60000 (98%)]\tLoss: 62.5 Average loss: 62.3 Test loss: 63.9\n",
      " Train Epoch: 95 [58880/60000 (98%)]\tLoss: 62.6 Average loss: 62.3 Test loss: 63.8\n",
      " Train Epoch: 96 [58880/60000 (98%)]\tLoss: 62.8 Average loss: 62.3 Test loss: 63.8\n",
      " Train Epoch: 97 [58880/60000 (98%)]\tLoss: 61.0 Average loss: 62.3 Test loss: 63.8\n",
      " Train Epoch: 98 [58880/60000 (98%)]\tLoss: 62.9 Average loss: 62.3 Test loss: 63.9\n",
      " Train Epoch: 99 [58880/60000 (98%)]\tLoss: 64.6 Average loss: 62.3 Test loss: 63.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 100 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 62.3 Test loss: 63.7\n",
      " Train Epoch: 101 [58880/60000 (98%)]\tLoss: 64.1 Average loss: 62.3 Test loss: 63.8\n",
      " Train Epoch: 102 [58880/60000 (98%)]\tLoss: 62.3 Average loss: 62.2 Test loss: 63.9\n",
      " Train Epoch: 103 [58880/60000 (98%)]\tLoss: 64.0 Average loss: 62.2 Test loss: 64.0\n",
      " Train Epoch: 104 [58880/60000 (98%)]\tLoss: 59.8 Average loss: 62.2 Test loss: 63.8\n",
      " Train Epoch: 105 [58880/60000 (98%)]\tLoss: 62.2 Average loss: 62.2 Test loss: 63.9\n",
      " Train Epoch: 106 [58880/60000 (98%)]\tLoss: 63.3 Average loss: 62.2 Test loss: 63.9\n",
      " Train Epoch: 107 [58880/60000 (98%)]\tLoss: 65.0 Average loss: 62.2 Test loss: 63.8\n",
      " Train Epoch: 108 [58880/60000 (98%)]\tLoss: 59.8 Average loss: 62.2 Test loss: 63.8\n",
      " Train Epoch: 109 [58880/60000 (98%)]\tLoss: 61.2 Average loss: 62.2 Test loss: 63.8\n",
      " Train Epoch: 110 [58880/60000 (98%)]\tLoss: 62.2 Average loss: 62.2 Test loss: 63.7\n",
      " Train Epoch: 111 [58880/60000 (98%)]\tLoss: 60.9 Average loss: 62.1 Test loss: 63.7\n",
      " Train Epoch: 112 [58880/60000 (98%)]\tLoss: 63.2 Average loss: 62.1 Test loss: 63.8\n",
      " Train Epoch: 113 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 62.1 Test loss: 63.7\n",
      " Train Epoch: 114 [58880/60000 (98%)]\tLoss: 62.3 Average loss: 62.1 Test loss: 63.9\n",
      " Train Epoch: 115 [58880/60000 (98%)]\tLoss: 58.7 Average loss: 62.1 Test loss: 63.8\n",
      " Train Epoch: 116 [58880/60000 (98%)]\tLoss: 63.6 Average loss: 62.1 Test loss: 63.8\n",
      " Train Epoch: 117 [58880/60000 (98%)]\tLoss: 61.7 Average loss: 62.1 Test loss: 63.8\n",
      " Train Epoch: 118 [58880/60000 (98%)]\tLoss: 59.5 Average loss: 62.1 Test loss: 63.8\n",
      " Train Epoch: 119 [58880/60000 (98%)]\tLoss: 61.1 Average loss: 62.0 Test loss: 63.8\n",
      " Train Epoch: 120 [58880/60000 (98%)]\tLoss: 61.6 Average loss: 62.1 Test loss: 63.8\n",
      " Train Epoch: 121 [58880/60000 (98%)]\tLoss: 60.2 Average loss: 62.0 Test loss: 63.7\n",
      " Train Epoch: 122 [58880/60000 (98%)]\tLoss: 61.1 Average loss: 62.0 Test loss: 63.7\n",
      " Train Epoch: 123 [58880/60000 (98%)]\tLoss: 64.6 Average loss: 62.0 Test loss: 63.7\n",
      " Train Epoch: 124 [58880/60000 (98%)]\tLoss: 64.3 Average loss: 62.0 Test loss: 63.8\n",
      " Train Epoch: 125 [58880/60000 (98%)]\tLoss: 60.0 Average loss: 62.0 Test loss: 63.7\n",
      " Train Epoch: 126 [58880/60000 (98%)]\tLoss: 62.1 Average loss: 62.0 Test loss: 63.7\n",
      " Train Epoch: 127 [58880/60000 (98%)]\tLoss: 62.0 Average loss: 62.0 Test loss: 63.7\n",
      " Train Epoch: 128 [58880/60000 (98%)]\tLoss: 62.0 Average loss: 62.0 Test loss: 63.8\n",
      " Train Epoch: 129 [58880/60000 (98%)]\tLoss: 62.1 Average loss: 62.0 Test loss: 63.8\n",
      " Train Epoch: 130 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 61.9 Test loss: 63.6\n",
      " Train Epoch: 131 [58880/60000 (98%)]\tLoss: 64.1 Average loss: 61.9 Test loss: 63.8\n",
      " Train Epoch: 132 [58880/60000 (98%)]\tLoss: 62.8 Average loss: 61.9 Test loss: 63.6\n",
      " Train Epoch: 133 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 134 [58880/60000 (98%)]\tLoss: 62.3 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 135 [58880/60000 (98%)]\tLoss: 59.7 Average loss: 61.9 Test loss: 63.6\n",
      " Train Epoch: 136 [58880/60000 (98%)]\tLoss: 61.7 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 137 [58880/60000 (98%)]\tLoss: 62.7 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 138 [58880/60000 (98%)]\tLoss: 64.3 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 139 [58880/60000 (98%)]\tLoss: 65.3 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 140 [58880/60000 (98%)]\tLoss: 64.7 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 141 [58880/60000 (98%)]\tLoss: 62.9 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 142 [58880/60000 (98%)]\tLoss: 62.1 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 143 [58880/60000 (98%)]\tLoss: 64.5 Average loss: 61.9 Test loss: 63.7\n",
      " Train Epoch: 144 [58880/60000 (98%)]\tLoss: 63.3 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 145 [58880/60000 (98%)]\tLoss: 62.8 Average loss: 61.8 Test loss: 63.8\n",
      " Train Epoch: 146 [58880/60000 (98%)]\tLoss: 63.4 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 147 [58880/60000 (98%)]\tLoss: 58.7 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 148 [58880/60000 (98%)]\tLoss: 62.1 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 149 [58880/60000 (98%)]\tLoss: 59.7 Average loss: 61.8 Test loss: 63.6\n",
      " Train Epoch: 150 [58880/60000 (98%)]\tLoss: 61.0 Average loss: 61.8 Test loss: 63.6\n",
      " Train Epoch: 151 [58880/60000 (98%)]\tLoss: 62.5 Average loss: 61.8 Test loss: 63.6\n",
      " Train Epoch: 152 [58880/60000 (98%)]\tLoss: 63.3 Average loss: 61.8 Test loss: 63.6\n",
      " Train Epoch: 153 [58880/60000 (98%)]\tLoss: 64.0 Average loss: 61.8 Test loss: 63.6\n",
      " Train Epoch: 154 [58880/60000 (98%)]\tLoss: 63.6 Average loss: 61.8 Test loss: 63.8\n",
      " Train Epoch: 155 [58880/60000 (98%)]\tLoss: 64.7 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 156 [58880/60000 (98%)]\tLoss: 64.0 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 157 [58880/60000 (98%)]\tLoss: 62.9 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 158 [58880/60000 (98%)]\tLoss: 62.5 Average loss: 61.8 Test loss: 63.7\n",
      " Train Epoch: 159 [58880/60000 (98%)]\tLoss: 62.6 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 160 [58880/60000 (98%)]\tLoss: 62.7 Average loss: 61.8 Test loss: 63.6\n",
      " Train Epoch: 161 [58880/60000 (98%)]\tLoss: 60.3 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 162 [58880/60000 (98%)]\tLoss: 61.3 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 163 [58880/60000 (98%)]\tLoss: 61.8 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 164 [58880/60000 (98%)]\tLoss: 61.2 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 165 [58880/60000 (98%)]\tLoss: 63.4 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 166 [58880/60000 (98%)]\tLoss: 62.5 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 167 [58880/60000 (98%)]\tLoss: 60.1 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 168 [58880/60000 (98%)]\tLoss: 58.2 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 169 [58880/60000 (98%)]\tLoss: 63.5 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 170 [58880/60000 (98%)]\tLoss: 62.8 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 171 [58880/60000 (98%)]\tLoss: 58.6 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 172 [58880/60000 (98%)]\tLoss: 62.0 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 173 [58880/60000 (98%)]\tLoss: 63.4 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 174 [58880/60000 (98%)]\tLoss: 58.1 Average loss: 61.7 Test loss: 63.8\n",
      " Train Epoch: 175 [58880/60000 (98%)]\tLoss: 60.4 Average loss: 61.7 Test loss: 63.6\n",
      " Train Epoch: 176 [58880/60000 (98%)]\tLoss: 61.1 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 177 [58880/60000 (98%)]\tLoss: 62.2 Average loss: 61.7 Test loss: 63.7\n",
      " Train Epoch: 178 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 179 [58880/60000 (98%)]\tLoss: 61.2 Average loss: 61.6 Test loss: 63.7\n",
      " Train Epoch: 180 [58880/60000 (98%)]\tLoss: 60.8 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 181 [58880/60000 (98%)]\tLoss: 62.4 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 182 [58880/60000 (98%)]\tLoss: 60.7 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 183 [58880/60000 (98%)]\tLoss: 62.1 Average loss: 61.6 Test loss: 63.7\n",
      " Train Epoch: 184 [58880/60000 (98%)]\tLoss: 61.8 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 185 [58880/60000 (98%)]\tLoss: 61.8 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 186 [58880/60000 (98%)]\tLoss: 63.9 Average loss: 61.6 Test loss: 63.7\n",
      " Train Epoch: 187 [58880/60000 (98%)]\tLoss: 62.0 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 188 [58880/60000 (98%)]\tLoss: 64.7 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 189 [58880/60000 (98%)]\tLoss: 58.9 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 190 [58880/60000 (98%)]\tLoss: 60.7 Average loss: 61.6 Test loss: 63.7\n",
      " Train Epoch: 191 [58880/60000 (98%)]\tLoss: 60.5 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 192 [58880/60000 (98%)]\tLoss: 59.8 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 193 [58880/60000 (98%)]\tLoss: 59.1 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 194 [58880/60000 (98%)]\tLoss: 60.9 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 195 [58880/60000 (98%)]\tLoss: 62.9 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 196 [58880/60000 (98%)]\tLoss: 63.5 Average loss: 61.6 Test loss: 63.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 197 [58880/60000 (98%)]\tLoss: 59.0 Average loss: 61.5 Test loss: 63.7\n",
      " Train Epoch: 198 [58880/60000 (98%)]\tLoss: 63.5 Average loss: 61.5 Test loss: 63.7\n",
      " Train Epoch: 199 [58880/60000 (98%)]\tLoss: 61.1 Average loss: 61.6 Test loss: 63.6\n",
      " Train Epoch: 200 [58880/60000 (98%)]\tLoss: 59.5 Average loss: 61.5 Test loss: 63.6\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "# digits in latent space\n",
    "# enregistrer en tant que matrice 8 × 8 = 64 de MNIST chiffres    \n",
    "    with torch.no_grad():\n",
    "     sample = torch.randn(64, 20).to(device)\n",
    "     sample = model.decode(sample).cpu()\n",
    "     save_image(sample.view(64, 1, 28, 28), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controle de results\n",
    "# dans fichier results : 200 images codees + 200 images decodees pour 200 epochs\n",
    "# sur chaque image (suite de 8 chiffres encodes > 8  chiffres decodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffb1c2f87f0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot losses\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d execution : 297.5 secondes ---\n"
     ]
    }
   ],
   "source": [
    "# Affichage du temps d execution\n",
    "print(\"Temps d execution : %s secondes ---\" % round((time.time() - start_time),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse : 5 mn avec 1 geforce gtx1080"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
