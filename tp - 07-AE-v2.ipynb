{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# Date : 02 dec. 2018\n",
    "# MS Valdom > apprenants > omar attaf, laurent lapasset, didier le picaut\n",
    "# Version = 2.0\n",
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syllabus - auto encoder (AE)\n",
    "# http://www.xavierdupre.fr/app/ensae_teaching_dl/helpsphinx//chapters/deep_apprentissage_sans_labels.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que sont les auto-encodeurs\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# == EXECUTIVE SUMMARY: \n",
    ".dataset mnist = train = 60 000 images 28x28 + test = 10 000 images 28 x 28\n",
    ".bien régler la taille du batch selon puissance GPU\n",
    "\n",
    "# - ARCHITECTURE 1\n",
    ".avec 200 Epochs, en test, loss criteria [stabilisee] = 63.6% - instance = 5mn  \n",
    ".modele rx neurones : 2 couches fully connected + 1 ReLU (entre)\n",
    ". (784,400) > (400,20)\n",
    "\n",
    "# - ARCHITECTURE 2**\n",
    ". loss criteria [stabilisee/test] = 57.9% - instance = 6.76mn\n",
    ". modele rx neurones : 4 couches \n",
    ". (784,400) > (400,128) > (128,64) > (64,32)\n",
    "\n",
    "# - ARCHITECTURE 3\n",
    ". loss criteria [stabilisee/test] = 62.5% - instance = 7.56mn\n",
    ". modele rx neurones : 5 couches\n",
    ". (784,400) > (400,256) > (256,128) > (128,64) > (64,32)\n",
    "\n",
    "# - ARCHITECTURE 4 \n",
    ".loss criteria [stabilisee/test] = 58.2% - instance  = 6.90mn\n",
    ". architecture 2 avec les parametres optimizer affines\n",
    "==> difficile a regler, besoin d une procedure validation croisee k-folds, gridsearch, des hyperparametres de l'optimiser Adam\n",
    "\n",
    "\n",
    "** : BEST SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://reyhaneaskari.github.io/AE.html\n",
    "# -------------------------------------------\n",
    "- Les encodeurs automatiques sont l’un des modèles d’apprentissage en profondeur \n",
    "non supervisé.\n",
    "- L'objectif d'un encodeur automatique est la réduction de la dimensionnalité \n",
    "et la découverte des fonctionnalités.\n",
    "- Un encodeur automatique est formé pour prévoir sa propre entrée, \n",
    "mais pour empêcher le modèle d’apprendre le mappage d’identité, \n",
    "certaines contraintes sont appliquées aux unités cachées."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/# \n",
    "# ----------------------------------\n",
    "Qu'est-ce qu'un autoencoder?\n",
    "L’idée générale de l’auto-encodeur (AE) est de faire passer les informations par un étroit goulet d’étranglement entre les parties encodeur (entrée) et décodeur (sortie) en miroir d’un réseau de neurones. (voir le schéma ci-dessous)\n",
    "\n",
    "Etant donné que l'architecture et la fonction de perte du réseau sont configurées de manière à ce que la sortie tente d'émuler l'entrée, le réseau doit apprendre à coder les données d'entrée sur l'espace très limité représenté par le goulot d'étranglement."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ". Les auto-encodeurs sont appris automatiquement à partir d’exemples de données, ce qui est une propriété utile: cela signifie qu’il est facile de former des instances spécialisées de l’algorithme qui fonctionneront bien avec un type d’entrée donné.\n",
    " \n",
    " Pour créer un auto-codeur, vous avez besoin de trois choses: \n",
    ". une fonction de codage, \n",
    ". une fonction de décodage \n",
    ". et une fonction de distance entre la quantité de perte d’information entre la représentation compressée de vos données et la représentation décompressée (c’est-à-dire une fonction de \"perte\"). \n",
    ". Le codeur et le décodeur seront choisis pour être des fonctions paramétriques (généralement des réseaux de neurones) et pour pouvoir être différenciés par rapport à la fonction de distance, de sorte que les paramètres des fonctions de codage / décodage puissent être optimisés pour minimiser la perte de reconstruction, en utilisant une descente de gradient stochastique.\n",
    "\n",
    "# --------------------------------\n",
    ". presque tous les contextes où le terme \"autoencoder\" est utilisé, \n",
    "les fonctions de compression et de décompression sont implémentées\n",
    "avec des réseaux de neurones.\n",
    "\n",
    ". les auto-encodeurs sont a perte, ie les sorties decompressees seront degradees par rapport aux entrees d'origine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# autocompletion\n",
    "# =================\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les libraries\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== mise en place d un compteur de temps pour voir le temps de convergence du Rx neurones [trop  ou pas ?]\n",
    "# Debut du decompte du temps\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : library cuda est-elle presente ?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#== sanity check : presence du framework cudnn ?\n",
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4c53480ed0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproductible\n",
    "SEED = 1\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "\n",
    "#epochs=10\n",
    "epochs=200\n",
    "\n",
    "loginterval=10\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== data chargement > dataset train + dataset test\n",
    "# Téléchargez ou chargez le jeu de données MNIST téléchargé \n",
    "# mélangez les données à chaque époque \n",
    "\n",
    "\n",
    "# Les instances de DataLoader chargeront les tenseurs directement dans la mémoire du GPU \n",
    "\n",
    "kwargs = {'num_workers': 11, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "# dataset train\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)\n",
    "\n",
    "# dataset test\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batchsize, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== transform=transforms.ToTensor()) ?\n",
    "\n",
    "# Convertit une image PIL ou numpy.ndarray (H x L x C) \n",
    "# comprise dans l'intervalle [0, 255] en une torche.\n",
    "# Tensoriseur de forme (C x H x W) compris dans l'intervalle [0.0, 1.0].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== creation du modele\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        \n",
    "# ENCODEUR \n",
    "# 28 x 28 pixels = 784 pixels en entrée, 400 sorties\n",
    "# couche unitaire linéaire rectifiée de 400 à 400 \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784,400),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(400,128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64,32) # espace latent = couche cachee\n",
    "            )\n",
    "\n",
    "# DECODEUR \n",
    "# de goulot d' étranglement à 400 caché\n",
    "# on fait le travail autre-sens\n",
    "    \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64,128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128,400),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(400,784),\n",
    "            )\n",
    "\n",
    "        # create using nn.Sequential()\n",
    "        # encoder :FullyConnected (784 -> 400), RELU activation; FC (400, 20)\n",
    "        # decoder :FullyConnected (20 -> 400), RELU activation; FC (400, 784)\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return torch.sigmoid(self.decoder(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xview = x.view((-1, 784))\n",
    "        z = self.encode(xview)\n",
    "        return self.decode(z)\n",
    "\n",
    "model = AE().to(device)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-06)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.view.html\n",
    "# otimizer utilise = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_batch, x):\n",
    "    \n",
    "    # compute bce as the binary cross entropy across the batch as a sum\n",
    "    # L'entrée est binarisée et Binary Cross Entropy a été utilisé comme fonction de perte.\n",
    "    # dans quelle mesure l'entrée x et la sortie recon_x sont-elles d'accord?\n",
    "    \n",
    "    loss = torch.nn.BCELoss(reduction='sum')\n",
    "    return loss(recon_batch, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch = model(data)\n",
    "        loss = loss_function(recon_batch, data.view(-1,784))\n",
    "        losses.append(loss.cpu().item())\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % loginterval == 0:\n",
    "            print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.1f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)), end='')\n",
    "\n",
    "    print(' Average loss: {:.1f}'.format(\n",
    "          train_loss / len(train_loader.dataset)), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch = model(data)\n",
    "            test_loss += loss_function(recon_batch, data.view(-1, 784))\n",
    "\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batchsize, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(' Test loss: {:.1f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results/reconstruction = les images sont reconstruites et images stockees ds le fichier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1 [58880/60000 (98%)]\tLoss: 133.4 Average loss: 183.1 Test loss: 133.4\n",
      " Train Epoch: 2 [58880/60000 (98%)]\tLoss: 105.2 Average loss: 120.1 Test loss: 110.8\n",
      " Train Epoch: 3 [58880/60000 (98%)]\tLoss: 106.9 Average loss: 106.0 Test loss: 101.1\n",
      " Train Epoch: 4 [58880/60000 (98%)]\tLoss: 88.0 Average loss: 98.2 Test loss: 94.4\n",
      " Train Epoch: 5 [58880/60000 (98%)]\tLoss: 91.5 Average loss: 93.0 Test loss: 90.7\n",
      " Train Epoch: 6 [58880/60000 (98%)]\tLoss: 89.7 Average loss: 89.0 Test loss: 87.2\n",
      " Train Epoch: 7 [58880/60000 (98%)]\tLoss: 82.1 Average loss: 86.0 Test loss: 84.6\n",
      " Train Epoch: 8 [58880/60000 (98%)]\tLoss: 87.2 Average loss: 83.3 Test loss: 81.6\n",
      " Train Epoch: 9 [58880/60000 (98%)]\tLoss: 81.9 Average loss: 80.9 Test loss: 79.6\n",
      " Train Epoch: 10 [58880/60000 (98%)]\tLoss: 77.8 Average loss: 79.2 Test loss: 78.7\n",
      " Train Epoch: 11 [58880/60000 (98%)]\tLoss: 76.5 Average loss: 77.9 Test loss: 77.4\n",
      " Train Epoch: 12 [58880/60000 (98%)]\tLoss: 77.4 Average loss: 76.9 Test loss: 76.5\n",
      " Train Epoch: 13 [58880/60000 (98%)]\tLoss: 77.2 Average loss: 76.0 Test loss: 76.0\n",
      " Train Epoch: 14 [58880/60000 (98%)]\tLoss: 75.7 Average loss: 75.1 Test loss: 75.1\n",
      " Train Epoch: 15 [58880/60000 (98%)]\tLoss: 72.9 Average loss: 74.2 Test loss: 73.9\n",
      " Train Epoch: 16 [58880/60000 (98%)]\tLoss: 69.2 Average loss: 73.4 Test loss: 73.2\n",
      " Train Epoch: 17 [58880/60000 (98%)]\tLoss: 71.4 Average loss: 72.6 Test loss: 73.1\n",
      " Train Epoch: 18 [58880/60000 (98%)]\tLoss: 69.9 Average loss: 71.9 Test loss: 72.2\n",
      " Train Epoch: 19 [58880/60000 (98%)]\tLoss: 70.0 Average loss: 71.3 Test loss: 71.5\n",
      " Train Epoch: 20 [58880/60000 (98%)]\tLoss: 70.4 Average loss: 70.9 Test loss: 71.1\n",
      " Train Epoch: 21 [58880/60000 (98%)]\tLoss: 71.6 Average loss: 70.3 Test loss: 70.9\n",
      " Train Epoch: 22 [58880/60000 (98%)]\tLoss: 66.8 Average loss: 69.9 Test loss: 70.2\n",
      " Train Epoch: 23 [58880/60000 (98%)]\tLoss: 65.4 Average loss: 69.5 Test loss: 69.9\n",
      " Train Epoch: 24 [58880/60000 (98%)]\tLoss: 68.1 Average loss: 69.1 Test loss: 69.6\n",
      " Train Epoch: 25 [58880/60000 (98%)]\tLoss: 68.2 Average loss: 68.8 Test loss: 69.3\n",
      " Train Epoch: 26 [58880/60000 (98%)]\tLoss: 66.8 Average loss: 68.3 Test loss: 69.1\n",
      " Train Epoch: 27 [58880/60000 (98%)]\tLoss: 69.9 Average loss: 68.0 Test loss: 68.7\n",
      " Train Epoch: 28 [58880/60000 (98%)]\tLoss: 71.4 Average loss: 67.7 Test loss: 68.8\n",
      " Train Epoch: 29 [58880/60000 (98%)]\tLoss: 66.1 Average loss: 67.4 Test loss: 68.3\n",
      " Train Epoch: 30 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 67.1 Test loss: 68.3\n",
      " Train Epoch: 31 [58880/60000 (98%)]\tLoss: 65.8 Average loss: 66.8 Test loss: 67.7\n",
      " Train Epoch: 32 [58880/60000 (98%)]\tLoss: 67.9 Average loss: 66.5 Test loss: 67.4\n",
      " Train Epoch: 33 [58880/60000 (98%)]\tLoss: 66.9 Average loss: 66.3 Test loss: 67.4\n",
      " Train Epoch: 34 [58880/60000 (98%)]\tLoss: 66.4 Average loss: 66.0 Test loss: 67.1\n",
      " Train Epoch: 35 [58880/60000 (98%)]\tLoss: 66.0 Average loss: 65.7 Test loss: 66.6\n",
      " Train Epoch: 36 [58880/60000 (98%)]\tLoss: 64.6 Average loss: 65.5 Test loss: 66.5\n",
      " Train Epoch: 37 [58880/60000 (98%)]\tLoss: 63.3 Average loss: 65.3 Test loss: 66.2\n",
      " Train Epoch: 38 [58880/60000 (98%)]\tLoss: 65.3 Average loss: 65.1 Test loss: 66.1\n",
      " Train Epoch: 39 [58880/60000 (98%)]\tLoss: 63.7 Average loss: 64.9 Test loss: 66.2\n",
      " Train Epoch: 40 [58880/60000 (98%)]\tLoss: 63.9 Average loss: 64.7 Test loss: 65.5\n",
      " Train Epoch: 41 [58880/60000 (98%)]\tLoss: 64.0 Average loss: 64.5 Test loss: 65.5\n",
      " Train Epoch: 42 [58880/60000 (98%)]\tLoss: 65.7 Average loss: 64.4 Test loss: 65.6\n",
      " Train Epoch: 43 [58880/60000 (98%)]\tLoss: 62.2 Average loss: 64.2 Test loss: 65.4\n",
      " Train Epoch: 44 [58880/60000 (98%)]\tLoss: 63.0 Average loss: 64.1 Test loss: 65.1\n",
      " Train Epoch: 45 [58880/60000 (98%)]\tLoss: 65.8 Average loss: 64.0 Test loss: 65.0\n",
      " Train Epoch: 46 [58880/60000 (98%)]\tLoss: 64.1 Average loss: 63.8 Test loss: 65.1\n",
      " Train Epoch: 47 [58880/60000 (98%)]\tLoss: 63.1 Average loss: 63.7 Test loss: 64.6\n",
      " Train Epoch: 48 [58880/60000 (98%)]\tLoss: 64.4 Average loss: 63.5 Test loss: 64.6\n",
      " Train Epoch: 49 [58880/60000 (98%)]\tLoss: 65.7 Average loss: 63.4 Test loss: 64.5\n",
      " Train Epoch: 50 [58880/60000 (98%)]\tLoss: 63.3 Average loss: 63.2 Test loss: 64.3\n",
      " Train Epoch: 51 [58880/60000 (98%)]\tLoss: 64.7 Average loss: 63.1 Test loss: 64.4\n",
      " Train Epoch: 52 [58880/60000 (98%)]\tLoss: 63.4 Average loss: 63.0 Test loss: 64.2\n",
      " Train Epoch: 53 [58880/60000 (98%)]\tLoss: 66.3 Average loss: 62.9 Test loss: 64.0\n",
      " Train Epoch: 54 [58880/60000 (98%)]\tLoss: 64.9 Average loss: 62.7 Test loss: 64.0\n",
      " Train Epoch: 55 [58880/60000 (98%)]\tLoss: 62.4 Average loss: 62.6 Test loss: 63.7\n",
      " Train Epoch: 56 [58880/60000 (98%)]\tLoss: 63.1 Average loss: 62.5 Test loss: 64.5\n",
      " Train Epoch: 57 [58880/60000 (98%)]\tLoss: 65.5 Average loss: 62.4 Test loss: 63.6\n",
      " Train Epoch: 58 [58880/60000 (98%)]\tLoss: 61.6 Average loss: 62.3 Test loss: 63.4\n",
      " Train Epoch: 59 [58880/60000 (98%)]\tLoss: 62.1 Average loss: 62.2 Test loss: 63.5\n",
      " Train Epoch: 60 [58880/60000 (98%)]\tLoss: 63.9 Average loss: 62.0 Test loss: 63.1\n",
      " Train Epoch: 61 [58880/60000 (98%)]\tLoss: 58.9 Average loss: 62.0 Test loss: 63.3\n",
      " Train Epoch: 62 [58880/60000 (98%)]\tLoss: 65.1 Average loss: 61.9 Test loss: 63.1\n",
      " Train Epoch: 63 [58880/60000 (98%)]\tLoss: 59.0 Average loss: 61.7 Test loss: 62.9\n",
      " Train Epoch: 64 [58880/60000 (98%)]\tLoss: 59.3 Average loss: 61.6 Test loss: 62.6\n",
      " Train Epoch: 65 [58880/60000 (98%)]\tLoss: 60.9 Average loss: 61.6 Test loss: 62.8\n",
      " Train Epoch: 66 [58880/60000 (98%)]\tLoss: 58.9 Average loss: 61.4 Test loss: 62.6\n",
      " Train Epoch: 67 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 61.4 Test loss: 62.2\n",
      " Train Epoch: 68 [58880/60000 (98%)]\tLoss: 60.5 Average loss: 61.3 Test loss: 62.3\n",
      " Train Epoch: 69 [58880/60000 (98%)]\tLoss: 59.6 Average loss: 61.2 Test loss: 62.3\n",
      " Train Epoch: 70 [58880/60000 (98%)]\tLoss: 61.3 Average loss: 61.1 Test loss: 62.4\n",
      " Train Epoch: 71 [58880/60000 (98%)]\tLoss: 62.4 Average loss: 61.0 Test loss: 62.4\n",
      " Train Epoch: 72 [58880/60000 (98%)]\tLoss: 60.7 Average loss: 60.9 Test loss: 62.4\n",
      " Train Epoch: 73 [58880/60000 (98%)]\tLoss: 61.1 Average loss: 60.9 Test loss: 61.9\n",
      " Train Epoch: 74 [58880/60000 (98%)]\tLoss: 58.5 Average loss: 60.8 Test loss: 61.9\n",
      " Train Epoch: 75 [58880/60000 (98%)]\tLoss: 60.0 Average loss: 60.7 Test loss: 61.9\n",
      " Train Epoch: 76 [58880/60000 (98%)]\tLoss: 59.7 Average loss: 60.7 Test loss: 62.0\n",
      " Train Epoch: 77 [58880/60000 (98%)]\tLoss: 59.5 Average loss: 60.6 Test loss: 61.7\n",
      " Train Epoch: 78 [58880/60000 (98%)]\tLoss: 59.2 Average loss: 60.5 Test loss: 61.7\n",
      " Train Epoch: 79 [58880/60000 (98%)]\tLoss: 63.6 Average loss: 60.4 Test loss: 61.6\n",
      " Train Epoch: 80 [58880/60000 (98%)]\tLoss: 59.4 Average loss: 60.4 Test loss: 61.5\n",
      " Train Epoch: 81 [58880/60000 (98%)]\tLoss: 57.7 Average loss: 60.3 Test loss: 61.7\n",
      " Train Epoch: 82 [58880/60000 (98%)]\tLoss: 57.2 Average loss: 60.2 Test loss: 61.3\n",
      " Train Epoch: 83 [58880/60000 (98%)]\tLoss: 62.3 Average loss: 60.2 Test loss: 61.4\n",
      " Train Epoch: 84 [58880/60000 (98%)]\tLoss: 60.6 Average loss: 60.1 Test loss: 61.2\n",
      " Train Epoch: 85 [58880/60000 (98%)]\tLoss: 61.5 Average loss: 60.0 Test loss: 61.1\n",
      " Train Epoch: 86 [58880/60000 (98%)]\tLoss: 61.3 Average loss: 60.0 Test loss: 61.3\n",
      " Train Epoch: 87 [58880/60000 (98%)]\tLoss: 60.1 Average loss: 60.0 Test loss: 61.1\n",
      " Train Epoch: 88 [58880/60000 (98%)]\tLoss: 60.9 Average loss: 59.9 Test loss: 61.0\n",
      " Train Epoch: 89 [58880/60000 (98%)]\tLoss: 60.4 Average loss: 59.8 Test loss: 61.3\n",
      " Train Epoch: 90 [58880/60000 (98%)]\tLoss: 59.9 Average loss: 59.8 Test loss: 61.1\n",
      " Train Epoch: 91 [58880/60000 (98%)]\tLoss: 59.6 Average loss: 59.7 Test loss: 60.9\n",
      " Train Epoch: 92 [58880/60000 (98%)]\tLoss: 60.3 Average loss: 59.7 Test loss: 61.4\n",
      " Train Epoch: 93 [58880/60000 (98%)]\tLoss: 58.1 Average loss: 59.7 Test loss: 60.9\n",
      " Train Epoch: 94 [58880/60000 (98%)]\tLoss: 60.0 Average loss: 59.6 Test loss: 61.0\n",
      " Train Epoch: 95 [58880/60000 (98%)]\tLoss: 58.7 Average loss: 59.5 Test loss: 60.9\n",
      " Train Epoch: 96 [58880/60000 (98%)]\tLoss: 61.2 Average loss: 59.5 Test loss: 60.8\n",
      " Train Epoch: 97 [58880/60000 (98%)]\tLoss: 61.9 Average loss: 59.4 Test loss: 61.0\n",
      " Train Epoch: 98 [58880/60000 (98%)]\tLoss: 58.8 Average loss: 59.4 Test loss: 60.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 99 [58880/60000 (98%)]\tLoss: 58.2 Average loss: 59.3 Test loss: 60.5\n",
      " Train Epoch: 100 [58880/60000 (98%)]\tLoss: 61.2 Average loss: 59.3 Test loss: 60.9\n",
      " Train Epoch: 101 [58880/60000 (98%)]\tLoss: 59.4 Average loss: 59.2 Test loss: 60.6\n",
      " Train Epoch: 102 [58880/60000 (98%)]\tLoss: 58.6 Average loss: 59.2 Test loss: 60.5\n",
      " Train Epoch: 103 [58880/60000 (98%)]\tLoss: 61.3 Average loss: 59.1 Test loss: 60.4\n",
      " Train Epoch: 104 [58880/60000 (98%)]\tLoss: 62.3 Average loss: 59.1 Test loss: 60.3\n",
      " Train Epoch: 105 [58880/60000 (98%)]\tLoss: 58.4 Average loss: 59.1 Test loss: 60.2\n",
      " Train Epoch: 106 [58880/60000 (98%)]\tLoss: 59.3 Average loss: 59.1 Test loss: 60.6\n",
      " Train Epoch: 107 [58880/60000 (98%)]\tLoss: 57.8 Average loss: 59.0 Test loss: 60.3\n",
      " Train Epoch: 108 [58880/60000 (98%)]\tLoss: 60.0 Average loss: 58.9 Test loss: 60.5\n",
      " Train Epoch: 109 [58880/60000 (98%)]\tLoss: 58.4 Average loss: 59.0 Test loss: 60.4\n",
      " Train Epoch: 110 [58880/60000 (98%)]\tLoss: 60.6 Average loss: 58.9 Test loss: 60.3\n",
      " Train Epoch: 111 [58880/60000 (98%)]\tLoss: 60.7 Average loss: 58.8 Test loss: 60.1\n",
      " Train Epoch: 112 [58880/60000 (98%)]\tLoss: 58.3 Average loss: 58.8 Test loss: 59.9\n",
      " Train Epoch: 113 [58880/60000 (98%)]\tLoss: 56.0 Average loss: 58.7 Test loss: 60.3\n",
      " Train Epoch: 114 [58880/60000 (98%)]\tLoss: 56.3 Average loss: 58.7 Test loss: 59.8\n",
      " Train Epoch: 115 [58880/60000 (98%)]\tLoss: 59.8 Average loss: 58.7 Test loss: 60.1\n",
      " Train Epoch: 116 [58880/60000 (98%)]\tLoss: 57.9 Average loss: 58.6 Test loss: 59.9\n",
      " Train Epoch: 117 [58880/60000 (98%)]\tLoss: 57.8 Average loss: 58.6 Test loss: 60.1\n",
      " Train Epoch: 118 [58880/60000 (98%)]\tLoss: 58.9 Average loss: 58.6 Test loss: 60.2\n",
      " Train Epoch: 119 [58880/60000 (98%)]\tLoss: 58.2 Average loss: 58.6 Test loss: 60.0\n",
      " Train Epoch: 120 [58880/60000 (98%)]\tLoss: 58.8 Average loss: 58.5 Test loss: 59.8\n",
      " Train Epoch: 121 [58880/60000 (98%)]\tLoss: 60.1 Average loss: 58.5 Test loss: 59.7\n",
      " Train Epoch: 122 [58880/60000 (98%)]\tLoss: 61.5 Average loss: 58.5 Test loss: 59.9\n",
      " Train Epoch: 123 [58880/60000 (98%)]\tLoss: 59.6 Average loss: 58.4 Test loss: 59.7\n",
      " Train Epoch: 124 [58880/60000 (98%)]\tLoss: 55.8 Average loss: 58.4 Test loss: 59.8\n",
      " Train Epoch: 125 [58880/60000 (98%)]\tLoss: 60.2 Average loss: 58.4 Test loss: 59.8\n",
      " Train Epoch: 126 [58880/60000 (98%)]\tLoss: 57.9 Average loss: 58.3 Test loss: 59.8\n",
      " Train Epoch: 127 [58880/60000 (98%)]\tLoss: 60.1 Average loss: 58.3 Test loss: 59.7\n",
      " Train Epoch: 128 [58880/60000 (98%)]\tLoss: 59.3 Average loss: 58.3 Test loss: 59.5\n",
      " Train Epoch: 129 [58880/60000 (98%)]\tLoss: 56.3 Average loss: 58.3 Test loss: 59.7\n",
      " Train Epoch: 130 [58880/60000 (98%)]\tLoss: 59.2 Average loss: 58.2 Test loss: 59.6\n",
      " Train Epoch: 131 [58880/60000 (98%)]\tLoss: 56.5 Average loss: 58.2 Test loss: 59.5\n",
      " Train Epoch: 132 [58880/60000 (98%)]\tLoss: 59.4 Average loss: 58.2 Test loss: 59.8\n",
      " Train Epoch: 133 [58880/60000 (98%)]\tLoss: 57.8 Average loss: 58.2 Test loss: 59.6\n",
      " Train Epoch: 134 [58880/60000 (98%)]\tLoss: 58.0 Average loss: 58.1 Test loss: 59.6\n",
      " Train Epoch: 135 [58880/60000 (98%)]\tLoss: 58.9 Average loss: 58.1 Test loss: 59.5\n",
      " Train Epoch: 136 [58880/60000 (98%)]\tLoss: 59.9 Average loss: 58.1 Test loss: 59.3\n",
      " Train Epoch: 137 [58880/60000 (98%)]\tLoss: 58.2 Average loss: 58.0 Test loss: 59.7\n",
      " Train Epoch: 138 [58880/60000 (98%)]\tLoss: 57.3 Average loss: 58.0 Test loss: 59.4\n",
      " Train Epoch: 139 [58880/60000 (98%)]\tLoss: 58.1 Average loss: 57.9 Test loss: 59.6\n",
      " Train Epoch: 140 [58880/60000 (98%)]\tLoss: 57.6 Average loss: 58.0 Test loss: 59.2\n",
      " Train Epoch: 141 [58880/60000 (98%)]\tLoss: 59.5 Average loss: 57.9 Test loss: 59.3\n",
      " Train Epoch: 142 [58880/60000 (98%)]\tLoss: 57.7 Average loss: 57.9 Test loss: 59.3\n",
      " Train Epoch: 143 [58880/60000 (98%)]\tLoss: 57.0 Average loss: 57.9 Test loss: 59.1\n",
      " Train Epoch: 144 [58880/60000 (98%)]\tLoss: 56.3 Average loss: 57.8 Test loss: 59.2\n",
      " Train Epoch: 145 [58880/60000 (98%)]\tLoss: 60.4 Average loss: 57.8 Test loss: 59.4\n",
      " Train Epoch: 146 [58880/60000 (98%)]\tLoss: 57.8 Average loss: 57.8 Test loss: 59.3\n",
      " Train Epoch: 147 [58880/60000 (98%)]\tLoss: 56.6 Average loss: 57.8 Test loss: 59.1\n",
      " Train Epoch: 148 [58880/60000 (98%)]\tLoss: 57.7 Average loss: 57.8 Test loss: 59.2\n",
      " Train Epoch: 149 [58880/60000 (98%)]\tLoss: 60.2 Average loss: 57.7 Test loss: 58.9\n",
      " Train Epoch: 150 [58880/60000 (98%)]\tLoss: 57.4 Average loss: 57.7 Test loss: 59.4\n",
      " Train Epoch: 151 [58880/60000 (98%)]\tLoss: 57.5 Average loss: 57.7 Test loss: 58.9\n",
      " Train Epoch: 152 [58880/60000 (98%)]\tLoss: 59.1 Average loss: 57.6 Test loss: 59.0\n",
      " Train Epoch: 153 [58880/60000 (98%)]\tLoss: 55.5 Average loss: 57.6 Test loss: 59.1\n",
      " Train Epoch: 154 [58880/60000 (98%)]\tLoss: 57.1 Average loss: 57.6 Test loss: 58.8\n",
      " Train Epoch: 155 [58880/60000 (98%)]\tLoss: 56.2 Average loss: 57.6 Test loss: 58.8\n",
      " Train Epoch: 156 [58880/60000 (98%)]\tLoss: 58.6 Average loss: 57.6 Test loss: 58.8\n",
      " Train Epoch: 157 [58880/60000 (98%)]\tLoss: 55.8 Average loss: 57.5 Test loss: 58.8\n",
      " Train Epoch: 158 [58880/60000 (98%)]\tLoss: 57.1 Average loss: 57.5 Test loss: 59.1\n",
      " Train Epoch: 159 [58880/60000 (98%)]\tLoss: 56.9 Average loss: 57.5 Test loss: 58.8\n",
      " Train Epoch: 160 [58880/60000 (98%)]\tLoss: 55.7 Average loss: 57.5 Test loss: 59.3\n",
      " Train Epoch: 161 [58880/60000 (98%)]\tLoss: 56.9 Average loss: 57.4 Test loss: 58.9\n",
      " Train Epoch: 162 [58880/60000 (98%)]\tLoss: 57.7 Average loss: 57.4 Test loss: 58.7\n",
      " Train Epoch: 163 [58880/60000 (98%)]\tLoss: 59.3 Average loss: 57.4 Test loss: 58.8\n",
      " Train Epoch: 164 [58880/60000 (98%)]\tLoss: 59.2 Average loss: 57.4 Test loss: 59.0\n",
      " Train Epoch: 165 [58880/60000 (98%)]\tLoss: 57.7 Average loss: 57.4 Test loss: 58.8\n",
      " Train Epoch: 166 [58880/60000 (98%)]\tLoss: 56.3 Average loss: 57.4 Test loss: 58.7\n",
      " Train Epoch: 167 [58880/60000 (98%)]\tLoss: 57.3 Average loss: 57.3 Test loss: 58.9\n",
      " Train Epoch: 168 [58880/60000 (98%)]\tLoss: 54.3 Average loss: 57.3 Test loss: 58.9\n",
      " Train Epoch: 169 [58880/60000 (98%)]\tLoss: 58.5 Average loss: 57.3 Test loss: 58.7\n",
      " Train Epoch: 170 [58880/60000 (98%)]\tLoss: 58.6 Average loss: 57.3 Test loss: 58.7\n",
      " Train Epoch: 171 [58880/60000 (98%)]\tLoss: 58.3 Average loss: 57.2 Test loss: 58.6\n",
      " Train Epoch: 172 [58880/60000 (98%)]\tLoss: 58.2 Average loss: 57.2 Test loss: 58.4\n",
      " Train Epoch: 173 [58880/60000 (98%)]\tLoss: 55.4 Average loss: 57.2 Test loss: 58.7\n",
      " Train Epoch: 174 [58880/60000 (98%)]\tLoss: 57.6 Average loss: 57.2 Test loss: 58.5\n",
      " Train Epoch: 175 [58880/60000 (98%)]\tLoss: 61.4 Average loss: 57.2 Test loss: 58.6\n",
      " Train Epoch: 176 [58880/60000 (98%)]\tLoss: 57.4 Average loss: 57.2 Test loss: 58.4\n",
      " Train Epoch: 177 [58880/60000 (98%)]\tLoss: 56.3 Average loss: 57.2 Test loss: 58.7\n",
      " Train Epoch: 178 [58880/60000 (98%)]\tLoss: 56.9 Average loss: 57.2 Test loss: 58.5\n",
      " Train Epoch: 179 [58880/60000 (98%)]\tLoss: 57.1 Average loss: 57.1 Test loss: 58.5\n",
      " Train Epoch: 180 [58880/60000 (98%)]\tLoss: 57.6 Average loss: 57.1 Test loss: 58.5\n",
      " Train Epoch: 181 [58880/60000 (98%)]\tLoss: 55.6 Average loss: 57.1 Test loss: 58.5\n",
      " Train Epoch: 182 [58880/60000 (98%)]\tLoss: 57.5 Average loss: 57.1 Test loss: 58.4\n",
      " Train Epoch: 183 [58880/60000 (98%)]\tLoss: 58.9 Average loss: 57.1 Test loss: 58.6\n",
      " Train Epoch: 184 [58880/60000 (98%)]\tLoss: 57.8 Average loss: 57.1 Test loss: 58.5\n",
      " Train Epoch: 185 [58880/60000 (98%)]\tLoss: 56.7 Average loss: 57.0 Test loss: 58.4\n",
      " Train Epoch: 186 [58880/60000 (98%)]\tLoss: 57.3 Average loss: 57.1 Test loss: 58.6\n",
      " Train Epoch: 187 [58880/60000 (98%)]\tLoss: 55.1 Average loss: 57.0 Test loss: 58.3\n",
      " Train Epoch: 188 [58880/60000 (98%)]\tLoss: 58.1 Average loss: 57.0 Test loss: 58.5\n",
      " Train Epoch: 189 [58880/60000 (98%)]\tLoss: 57.9 Average loss: 57.0 Test loss: 59.0\n",
      " Train Epoch: 190 [58880/60000 (98%)]\tLoss: 57.3 Average loss: 57.0 Test loss: 58.3\n",
      " Train Epoch: 191 [58880/60000 (98%)]\tLoss: 56.8 Average loss: 56.9 Test loss: 58.4\n",
      " Train Epoch: 192 [58880/60000 (98%)]\tLoss: 57.4 Average loss: 56.9 Test loss: 58.3\n",
      " Train Epoch: 193 [58880/60000 (98%)]\tLoss: 55.0 Average loss: 57.0 Test loss: 58.3\n",
      " Train Epoch: 194 [58880/60000 (98%)]\tLoss: 56.3 Average loss: 57.0 Test loss: 58.4\n",
      " Train Epoch: 195 [58880/60000 (98%)]\tLoss: 58.1 Average loss: 56.9 Test loss: 58.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 196 [58880/60000 (98%)]\tLoss: 56.9 Average loss: 56.9 Test loss: 58.3\n",
      " Train Epoch: 197 [58880/60000 (98%)]\tLoss: 53.8 Average loss: 56.9 Test loss: 58.3\n",
      " Train Epoch: 198 [58880/60000 (98%)]\tLoss: 56.4 Average loss: 56.9 Test loss: 58.4\n",
      " Train Epoch: 199 [58880/60000 (98%)]\tLoss: 57.0 Average loss: 56.9 Test loss: 58.3\n",
      " Train Epoch: 200 [58880/60000 (98%)]\tLoss: 57.6 Average loss: 56.8 Test loss: 58.2\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    \n",
    "# digits in latent space\n",
    "# enregistrer en tant que matrice 8 × 8 = 64 de MNIST chiffres    \n",
    "    with torch.no_grad():\n",
    "     #sample = torch.randn(64, 20).to(device)\n",
    "     sample = torch.randn(64, 32).to(device)\n",
    "     sample = model.decode(sample).cpu()\n",
    "     save_image(sample.view(64, 1, 28, 28), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#== ARCHITECTURE 1 \n",
    "# controle de results\n",
    "# dans fichier results : 200 images codees + 200 images decodees pour\n",
    "# 200 epochs\n",
    "# sur chaque image (suite de 8 chiffres encodes > 8  chiffres decodes)\n",
    "\n",
    "# La rangée supérieure correspond aux chiffres d'origine et \n",
    "# la rangée inférieure aux chiffres reconstruits. \n",
    "# Nous perdons pas mal de détails avec cette approche de base.\n",
    "\n",
    "#== ARCHITECTURE 2\n",
    "# le contrôle des images des chiffres reconstruits, montre qu'avec l'architecture 2, \n",
    "# les chiffres reconstruits sont \"un peu meilleurs\" (aligne avec loss = 57.9%)\n",
    "\n",
    "#== ARCHITECTURE 3\n",
    "# loss = 62.5% - test\n",
    "\n",
    "#== ARCHICTECTURE 4 \n",
    "# loss = 58.2% - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c001e3c50>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot losses\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d execution : 414.5 secondes ---\n"
     ]
    }
   ],
   "source": [
    "# Affichage du temps d execution\n",
    "print(\"Temps d execution : %s secondes ---\" % round((time.time() - start_time),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSE :\n",
    "# architecture 1 = 5 mn avec 1 geforce gtx1080\n",
    "# architecture 2 ~ 6.76 mn\n",
    "# architecture 3 ~ 7.56 mn\n",
    "# architecture 4 ~ 6.90 mn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
